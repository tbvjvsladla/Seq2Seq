{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from NLP_pp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서ID</th>\n",
       "      <th>번역</th>\n",
       "      <th>원문(src)</th>\n",
       "      <th>번역문(tar)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ot_100615</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>mal sehen , ob meine haut reagiert oder nicht .</td>\n",
       "      <td>내 피부가 반응하는지 보자 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ot_100616</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>ich habe viele großartige kritiken über drunk ...</td>\n",
       "      <td>저는 드렁크 엘리펀트에 대한 훌륭한 리뷰를 많이 들었습니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ot_100617</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>lass uns zur poolparty gehen .</td>\n",
       "      <td>수영장 파티에 가자 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ot_100618</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>sie bringen einen neuen reiniger auf den markt .</td>\n",
       "      <td>그들은 새로운 청소기를 시장에서 출시하고 있어요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ot_100619</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>genau hier ist nur gepresster gemüsesaft .</td>\n",
       "      <td>바로 여기에 압착 야채 주스가 있어요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120327</th>\n",
       "      <td>md_100608</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>er sucht nach dem ort , der die bewohner verei...</td>\n",
       "      <td>그는 주민들을 결집시킬 수 있는 장소를 찾아다녔고 , 이를 빵집에서 찾는다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120328</th>\n",
       "      <td>md_100609</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>coelho costinha ist ein einfacher mann , der j...</td>\n",
       "      <td>코엘호 코스틴하는 재정적인 어려움을 겪고 있는 평범한 사람이다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120329</th>\n",
       "      <td>md_100610</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>als sie aufwacht , ist sie doppelt so alt , de...</td>\n",
       "      <td>그녀가 깨어났을 때 , 그녀는 나이가 두 배나 많이 들었지만 , 여전히 10대처럼 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120330</th>\n",
       "      <td>md_100611</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>viele frauen stecken in der misere , und alle ...</td>\n",
       "      <td>많은 여성들이 불행 속에 살고 있고 , 그들 모두는 그들의 상황에서 벗어날 길을 찾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120331</th>\n",
       "      <td>md_100614</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>eines tages erfährt corvaz , dass sich seine m...</td>\n",
       "      <td>어느 날 , 코바즈는 그의 어머니가 자살했다는 것을 알게 된다 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120332 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             문서ID    번역                                            원문(src)  \\\n",
       "0       ot_100615  독->한    mal sehen , ob meine haut reagiert oder nicht .   \n",
       "1       ot_100616  독->한  ich habe viele großartige kritiken über drunk ...   \n",
       "2       ot_100617  독->한                     lass uns zur poolparty gehen .   \n",
       "3       ot_100618  독->한   sie bringen einen neuen reiniger auf den markt .   \n",
       "4       ot_100619  독->한         genau hier ist nur gepresster gemüsesaft .   \n",
       "...           ...   ...                                                ...   \n",
       "120327  md_100608  독->한  er sucht nach dem ort , der die bewohner verei...   \n",
       "120328  md_100609  독->한  coelho costinha ist ein einfacher mann , der j...   \n",
       "120329  md_100610  독->한  als sie aufwacht , ist sie doppelt so alt , de...   \n",
       "120330  md_100611  독->한  viele frauen stecken in der misere , und alle ...   \n",
       "120331  md_100614  독->한  eines tages erfährt corvaz , dass sich seine m...   \n",
       "\n",
       "                                                 번역문(tar)  \n",
       "0                                        내 피부가 반응하는지 보자 .  \n",
       "1                      저는 드렁크 엘리펀트에 대한 훌륭한 리뷰를 많이 들었습니다 .  \n",
       "2                                            수영장 파티에 가자 .  \n",
       "3                            그들은 새로운 청소기를 시장에서 출시하고 있어요 .  \n",
       "4                                  바로 여기에 압착 야채 주스가 있어요 .  \n",
       "...                                                   ...  \n",
       "120327        그는 주민들을 결집시킬 수 있는 장소를 찾아다녔고 , 이를 빵집에서 찾는다 .  \n",
       "120328               코엘호 코스틴하는 재정적인 어려움을 겪고 있는 평범한 사람이다 .  \n",
       "120329  그녀가 깨어났을 때 , 그녀는 나이가 두 배나 많이 들었지만 , 여전히 10대처럼 ...  \n",
       "120330  많은 여성들이 불행 속에 살고 있고 , 그들 모두는 그들의 상황에서 벗어날 길을 찾...  \n",
       "120331               어느 날 , 코바즈는 그의 어머니가 자살했다는 것을 알게 된다 .  \n",
       "\n",
       "[120332 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('exam_deko.csv')\n",
    "\n",
    "display(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더의 입력 데이터인 src 데이터\n",
    "raw_src_data = raw_data['원문(src)'].values.tolist()\n",
    "# 디코더의 정답지 데이터인 tar 데이터\n",
    "raw_tar_data = raw_data['번역문(tar)'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 독일어 토크나이저 모델: dbmdz/bert-base-german-uncased\n",
    "german_tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\")\n",
    "# 한국어 토크나이저 모델: kykim/bert-kor-base\n",
    "korean_tokenizer = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "토큰화 진행 중: 100%|██████████| 120332/120332 [00:21<00:00, 5544.77it/s]\n",
      "토큰화 진행 중: 100%|██████████| 120332/120332 [00:12<00:00, 9497.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# 원문 토큰화 수행\n",
    "tokenized_src_data = tokenize(raw_src_data, german_tokenizer, arch='Bert')\n",
    "# 번역문 토큰화 수행\n",
    "tokenized_tar_data = tokenize(raw_tar_data, korean_tokenizer, arch='Bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련/검증/평가를 80%, 15%, 5%로 분할을 수행\n",
    "# random_state -> 데이터셋을 내누는데 '재현성' 유지를 위해 넣음 -> 안넣어도 됨\n",
    "# stratify -> y 클래스 비율을 알기 어렵기에 해당 항목은 없앰\n",
    "src_train, src_etc, tar_train, tar_etc = train_test_split(\n",
    "    tokenized_src_data, tokenized_tar_data, test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 그 외 데이터셋을 반반으로 Val, Test로 나눔\n",
    "src_val, src_test, tar_val, tar_test = train_test_split(\n",
    "    src_etc, tar_etc, test_size=0.25,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "src_word_list = []\n",
    "tar_word_list = []\n",
    "# train항목을 워드 리스트에 입력\n",
    "for src_sent, tar_sent in zip(src_train, tar_train):\n",
    "    for word in src_sent:\n",
    "        src_word_list.append(word)\n",
    "    for word in tar_sent:\n",
    "        tar_word_list.append(word)\n",
    "# val항목을 워드 리스트에 입력\n",
    "for src_sent, tar_sent in zip(src_train, tar_train):\n",
    "    for word in src_sent:\n",
    "        src_word_list.append(word)\n",
    "    for word in tar_sent:\n",
    "        tar_word_list.append(word)\n",
    "\n",
    "# 단어와 해당 단어의 출몰 빈도를 함께 저장하는\n",
    "# Counter 타입의 변수 생성\n",
    "src_word_counts = Counter(src_word_list)\n",
    "tar_word_counts = Counter(tar_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---원문(src)에 대한 희소단어 분석---\n",
      "\n",
      "---번역문(tar)에 대한 희소단어 분석---\n"
     ]
    }
   ],
   "source": [
    "rare_th = 0 #희소단어의 등장 빈도를 결정하는 파라미터\n",
    "# 희소단어 등장 빈도를 바탕으로 희소 단어를 배제하기 위해 준비 함수\n",
    "print(f'---원문(src)에 대한 희소단어 분석---')\n",
    "src_tot_vocab_cnt, src_rare_vocab_cnt = set_rare_vocab(src_word_counts, rare_th)\n",
    "print(f'\\n---번역문(tar)에 대한 희소단어 분석---')\n",
    "tar_tot_vocab_cnt, tar_rare_vocab_cnt = set_rare_vocab(tar_word_counts, rare_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#등장 빈도가 높은 단어 순으로 정렬하기\n",
    "src_vocab = sorted(src_word_counts, key=src_word_counts.get, reverse=True)\n",
    "tar_vocab = sorted(tar_word_counts, key=tar_word_counts.get, reverse=True)\n",
    "\n",
    "# 원문(src)에 대한 희소단어 배제 & 정렬 작업 수행\n",
    "src_vocab_size = src_tot_vocab_cnt - src_rare_vocab_cnt\n",
    "src_vocab = src_vocab[:src_vocab_size]\n",
    "# 번역문(tar에 대한 희소단어 배제 & 정렬 작업 수행\n",
    "tar_vocab_size = tar_tot_vocab_cnt - tar_rare_vocab_cnt\n",
    "tar_vocab = tar_vocab[:tar_vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---원문(src)에 대한 단어장 분석---\n",
      "단어집합(vocab)은 word_to_idx를 통해서\n",
      "[단어 : idx]의 <class 'dict'>타입이 되고\n",
      "스페셜 토큰 <PAD> <UNK> <SOS> <EOS> 을 포함하여\n",
      "총 관리되는 단어 '22471' -> '22475'가 됨\n",
      "\n",
      "---번역문(tar)에 대한 단어장 분석---\n",
      "단어집합(vocab)은 word_to_idx를 통해서\n",
      "[단어 : idx]의 <class 'dict'>타입이 되고\n",
      "스페셜 토큰 <PAD> <UNK> <SOS> <EOS> 을 포함하여\n",
      "총 관리되는 단어 '25697' -> '25701'가 됨\n"
     ]
    }
   ],
   "source": [
    "# 스페셜 토큰 선언\n",
    "spec_token = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "# 스페셜 토큰을 포함한 {단어:단어idx}의 딕셔너리 생성\n",
    "print(f'---원문(src)에 대한 단어장 분석---')\n",
    "src_to_idx, idx_to_src = set_word_to_idx(spec_token, src_vocab, \n",
    "                                         report=True)\n",
    "print(f'\\n---번역문(tar)에 대한 단어장 분석---')\n",
    "tar_to_idx, idx_to_tar = set_word_to_idx(spec_token, tar_vocab, \n",
    "                                         report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문(src) 데이터셋의 정수 인코딩 수행\n",
    "e_src_train = text_to_sequences(src_train, src_to_idx)\n",
    "e_src_val = text_to_sequences(src_val, src_to_idx)\n",
    "e_src_test = text_to_sequences(src_test, src_to_idx)\n",
    "\n",
    "# 번역문(tar) 데이터셋의 정수 인코딩 수행\n",
    "e_tar_train = text_to_sequences(tar_train, tar_to_idx)\n",
    "e_tar_val = text_to_sequences(tar_val, tar_to_idx)\n",
    "e_tar_test = text_to_sequences(tar_test, tar_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문(tar)의 접두/접미에 SOS, EOS 토큰 추가\n",
    "e_tar_train = prefix_suffix_token_insert(e_tar_train, spec_token)\n",
    "e_tar_val = prefix_suffix_token_insert(e_tar_val, spec_token)\n",
    "e_tar_test = prefix_suffix_token_insert(e_tar_test, spec_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 문장 길이가 60 이하 데이터 비율: 100.00%\n",
      "데이터셋 문장 길이가 55 이하 데이터 비율: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 원문(src)에 대한 문장패딩 하이퍼 파라미터 설정\n",
    "src_seq_len = 60\n",
    "set_sent_pad(e_src_train, src_seq_len)\n",
    "\n",
    "# 번역문(tar)에 대한 문장패딩 하이퍼 파라미터 설정\n",
    "tar_seq_len = 55\n",
    "set_sent_pad(e_tar_train, tar_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문(src)의 문장 패딩(정수인코딩 완료)\n",
    "padded_src_train = pad_seq_x(e_src_train, src_seq_len)\n",
    "padded_src_val = pad_seq_x(e_src_val, src_seq_len)\n",
    "padded_src_test = pad_seq_x(e_src_test, src_seq_len)\n",
    "\n",
    "# 번역문(tar)의 문장 패딩(정수인코딩 완료)\n",
    "padded_tar_train = pad_seq_x(e_tar_train, tar_seq_len)\n",
    "padded_tar_val = pad_seq_x(e_tar_val, tar_seq_len)\n",
    "padded_tar_test = pad_seq_x(e_tar_test, tar_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---원문(src) 문장 패딩 결과---\n",
      "훈련용 정수(원핫)인코딩 shape: (96265, 60)\n",
      "검증용 정수(원핫)인코딩 shape: (18050, 60)\n",
      "평가용 정수(원핫)인코딩 shape: (6017, 60)\n",
      "\n",
      "---번역문(tar) 문장 패딩 결과---\n",
      "훈련용 정수(원핫)인코딩 shape: (96265, 55)\n",
      "검증용 정수(원핫)인코딩 shape: (18050, 55)\n",
      "평가용 정수(원핫)인코딩 shape: (6017, 55)\n"
     ]
    }
   ],
   "source": [
    "print(f'---원문(src) 문장 패딩 결과---')\n",
    "val_pad_shape(padded_src_train, '훈련')\n",
    "val_pad_shape(padded_src_val, '검증')\n",
    "val_pad_shape(padded_src_test, '평가')\n",
    "print(f'\\n---번역문(tar) 문장 패딩 결과---')\n",
    "val_pad_shape(padded_tar_train, '훈련')\n",
    "val_pad_shape(padded_tar_val, '검증')\n",
    "val_pad_shape(padded_tar_test, '평가')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 X(인코딩)데이터 크기: [96265, 60]\n",
      "훈련용 Y(Label)데이터 크기: [96265, 55]\n",
      "검증용 X(인코딩)데이터 크기: [18050, 60]\n",
      "검증용 Y(Label)데이터 크기: [18050, 55]\n",
      "평가용 X(인코딩)데이터 크기: [6017, 60]\n",
      "평가용 Y(Label)데이터 크기: [6017, 55]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "bs = 256 # Batch_size 하이퍼 파라미터\n",
    "\n",
    "# 정수(원핫)인코딩 데이터를 데이터로더로 변환\n",
    "trainloader = set_dataloader(padded_src_train, padded_tar_train, bs, \n",
    "                             content='훈련', report=True)\n",
    "valloader = set_dataloader(padded_src_val, padded_tar_val, bs,\n",
    "                           content='검증', report=True)\n",
    "testloader = set_dataloader(padded_src_test, padded_tar_test, bs, \n",
    "                            content='평가', report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 하이퍼 파라미터 정리\n",
    "src_VS = len(src_to_idx) # 원문 단어장 개수\n",
    "tar_VS = len(tar_to_idx) # 번역문 단어장 개수\n",
    "src_SL = src_seq_len # 원문의 문장 길이\n",
    "# 번역문의 문장 길이를 디코더(생성)문장 길이로 쓰자\n",
    "tar_SL = tar_seq_len # 번역문 문장 길이\n",
    "\n",
    "EMB_DIM = 384 # 인코더/디코더의 임베딩 레이어 차원\n",
    "# unit_dim은 인코더-디코더 사이의 히든레이어처럼 생각하는게 편함\n",
    "UNIT_DIM = 256 # 인코더와 디코더의 rnn_out 차원값\n",
    "\n",
    "NUM_Layers = 2 # 인코더/디코더의 셀은 2층으로 만들자\n",
    "BI_DIR = False # 단방향으로만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------+\n",
      "| 하이퍼 파라미터         | 값          |\n",
      "+=========================+=============+\n",
      "| 원문 단어장 개수        | 22475개     |\n",
      "+-------------------------+-------------+\n",
      "| 번역문 단어장 개수      | 25701개     |\n",
      "+-------------------------+-------------+\n",
      "| 원문 문장 길이          | 60토큰      |\n",
      "+-------------------------+-------------+\n",
      "| 번역문 문장 길이        | 55토큰      |\n",
      "+-------------------------+-------------+\n",
      "| 원문/번역문 임베딩 차원 | 384         |\n",
      "+-------------------------+-------------+\n",
      "| 인코더-디코더 연결 차원 | 256         |\n",
      "+-------------------------+-------------+\n",
      "| 셀 층 개수              | 2층         |\n",
      "+-------------------------+-------------+\n",
      "| 양방향/단방향           | 단방향 학습 |\n",
      "+-------------------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# 출력할 데이터를 리스트 형식으로 준비\n",
    "data = [\n",
    "    [\"원문 단어장 개수\", f\"{src_VS}개\"],\n",
    "    [\"번역문 단어장 개수\", f\"{tar_VS}개\"],\n",
    "    [\"원문 문장 길이\", f\"{src_SL}토큰\"],\n",
    "    [\"번역문 문장 길이\", f\"{tar_SL}토큰\"],\n",
    "    [\"원문/번역문 임베딩 차원\", EMB_DIM],\n",
    "    [\"인코더-디코더 연결 차원\", UNIT_DIM],\n",
    "    [\"셀 층 개수\", f\"{NUM_Layers}층\"],\n",
    "    [\"양방향/단방향\", \"단방향 학습\"],\n",
    "]\n",
    "\n",
    "# 표 형식으로 출력\n",
    "print(tabulate(data, tablefmt=\"grid\",\n",
    "        headers=[\"하이퍼 파라미터\", \"값\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, rnn_cell, # 백본 셀이 어떤 종류인지 정의\n",
    "                 src_vocab, src_emb_dim, rnn_dim, \n",
    "                 num_layer=1, bi=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        # <PAD>토큰의 인덱싱을 지정하면 해당 idx(0)은\n",
    "        # word_vector을 만들 때 모두 0으로 채워지게 만들어준다.\n",
    "        self.embed = nn.Embedding(src_vocab, src_emb_dim,\n",
    "                                  padding_idx=0)\n",
    "        # 백본 셀을 LSTM / GRU중 선택하기\n",
    "        if rnn_cell.lower() == 'lstm':\n",
    "            self.rnn_cell = nn.LSTM\n",
    "        elif rnn_cell.lower() == 'gru':\n",
    "            self.rnn_cell = nn.GRU\n",
    "        else:\n",
    "            raise ValueError(\"백본 셀 잘못 입력\")\n",
    "        self.rnn = self.rnn_cell(input_size=src_emb_dim,#언어모델 입력차원\n",
    "                                hidden_size=rnn_dim,   #언어모델 출력차원\n",
    "                                num_layers=num_layer,  #언어모델 몇층?\n",
    "                                bidirectional=bi,      #양방향학습 On?\n",
    "                                batch_first=True)      #왠만하면 True\n",
    "    \n",
    "    def forward(self, x): # x의 차원 : (BS, src_seq_len)\n",
    "        emb = self.embed(x) # (BS, src_seq_len, src_emb_dim)\n",
    "        # 인코더에 양방향 학습을 적용한다\n",
    "        # rnn_out : (bs, src_seq_len, hidden_dim * 2)\n",
    "        # hidden : (num_layer * 2, bs, hidden_dim)\n",
    "        rnn_out , hidden = self.rnn(emb)\n",
    "        #인코더의 출력은 context_vector\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, rnn_cell, # 백본 셀이 어떤 종류인지 정의\n",
    "                 tar_vocab, tar_emb_dim, rnn_dim,\n",
    "                 num_layer=1, bi=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        # <PAD>토큰의 인덱싱을 지정하면 해당 idx(0)은\n",
    "        # word_vector을 만들 때 모두 0으로 채워지게 만들어준다.\n",
    "        self.embed = nn.Embedding(tar_vocab, tar_emb_dim,\n",
    "                                  padding_idx=0)\n",
    "        # 백본 셀을 LSTM / GRU중 선택하기\n",
    "        if rnn_cell.lower() == 'lstm':\n",
    "            self.rnn_cell = nn.LSTM\n",
    "        elif rnn_cell.lower() == 'gru':\n",
    "            self.rnn_cell = nn.GRU\n",
    "        else:\n",
    "            raise ValueError(\"백본 셀 잘못 입력\")\n",
    "        self.rnn = self.rnn_cell(input_size=tar_emb_dim,#언어모델 입력차원\n",
    "                                hidden_size=rnn_dim,   #언어모델 출력차원\n",
    "                                num_layers=num_layer,  #언어모델 몇층?\n",
    "                                bidirectional=bi,      #양방향학습 On?\n",
    "                                batch_first=True)      #왠만하면 True)\n",
    "        # 디코더의 출력은 정답(번역문)의 seq_len이 되게 해야함\n",
    "        # 맞춰야 하는 클래스 개수는 정답지의 단어 개수임\n",
    "        if bi : #양방향으로 학습시에는 FC 레이어 입력차원이 두배\n",
    "            self.fc = nn.Linear(rnn_dim*2, tar_vocab)\n",
    "        else:\n",
    "            self.fc = nn.Linear(rnn_dim, tar_vocab)\n",
    "    \n",
    "    # 디코더는 인코더의 context_vector을 초기 hidden으로 입력받는다.\n",
    "    def forward(self, x, hidden): # x의 차원 : (BS, tar_seq_len)\n",
    "        emb = self.embed(x) # (BS, tar_seq_len, tar_emb_dim)\n",
    "        # 디코더에 양방향 학습을 적용한다\n",
    "        # rnn_out : (bs, tar_seq_len, hidden_dim * 2)\n",
    "        # hidden : (num_layer * 2, bs, hidden_dim)\n",
    "        rnn_out, hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        output = self.fc(rnn_out)\n",
    "        # 최종 출력은 (bs, seq_len, tar_vocab)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, spec_token, max_len=None):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encode = encoder\n",
    "        self.decode = decoder\n",
    "        # 최대 디코딩 길이 설정\n",
    "        self.max_len = max_len\n",
    "        # 스페셜 토큰을 초기화에 입력하게 변경\n",
    "        self.spec_token = spec_token\n",
    "\n",
    "    def _devide_data(self, hidden):\n",
    "        h, c = hidden\n",
    "        return h, c\n",
    "    def _combine_data(self, hidden, cell):\n",
    "        context_vector = hidden, cell\n",
    "        return context_vector\n",
    "    def _track_hidden(self, hidden, score_idx, BW):\n",
    "        #입력 히든차원 :(nL, BW*BS, hid_dim)\n",
    "        #입력 추적벡터 차원 : (BW, BS, 1)\n",
    "        h = hidden.permute(1, 0, 2) # (BW*BS, nL, hid_dim)\n",
    "        temp, nl, hid_dim = h.size() #temp는 BW*BS임\n",
    "        BS = temp // BW\n",
    "        # (BW, BS, 1) -> (BW*BS, 1, 1) -> (BW*BS, nL, hid_dim)\n",
    "        track_idx = (score_idx//BW).reshape(-1, 1, 1).expand(-1, nl, hid_dim)\n",
    "        update_h = torch.gather(h, 0, track_idx)\n",
    "        # (BW*BS, nL, hid_dim) -> (nL, BW*BS, hid_dim) -> (BW, nL, BS, hid_dim)\n",
    "        h = update_h.permute(1, 0, 2).reshape(BW, nl, BS, hid_dim)\n",
    "        return h\n",
    "\n",
    "    def forward(self, src, tar=None, TF_ratio=1, BW=1):\n",
    "        # 인코더의 출력 = context_vector\n",
    "        context_h = self.encode(src)\n",
    "\n",
    "        # 스페셜 토큰에서 SOS, EOS, PAD의 정수인코딩값 추출\n",
    "        en_sos = self.spec_token.index('<SOS>')\n",
    "        en_eos = self.spec_token.index('<EOS>')\n",
    "        en_pad = self.spec_token.index('<PAD>')\n",
    "\n",
    "        if tar is not None:\n",
    "            # 배치사이즈, 연산위치 정보 추출(tar 기준으로)\n",
    "            BS, tar_seq_len = tar.size() \n",
    "            device = tar.device\n",
    "\n",
    "            # 디코더의 첫번째 토큰을 <SOS>에 (BS, 1)차원으로 생성\n",
    "            # 이때 정답지(tar)의 맨 앞토큰은 <SOS>로 채워져 있으니 이를 이용한다.\n",
    "            input_token = tar[:, 0].unsqueeze(1)\n",
    "            outputs = [] # 출력 디코드 결과를 저장\n",
    "\n",
    "            # 임의 난수를 (BS)차원으로 생성 후 TF_ratio비율정보를 받아서\n",
    "            # 마스크 플래그로 변환, 이때 TF_ratio는 0~1 사이값\n",
    "            # 1에 가까울수록 대부분의 Flag는 True가 되서 지도학습비율이 올라감\n",
    "            TF_flag = torch.rand(BS, device=device) < TF_ratio\n",
    "\n",
    "            # 토큰 단위로 예측이니 자가 회귀 방식임\n",
    "            h = context_h\n",
    "            # tar seq는 맨 처음 토큰을 <SOS>로 채웟으니 1번부터 시작\n",
    "            for t in range(1, tar_seq_len):\n",
    "                # 토큰단위로 입력이니 출력은 (bs, 1, tar_vocab)이다.\n",
    "                out_tokens, h = self.decode(input_token, h)\n",
    "                outputs.append(out_tokens)\n",
    "\n",
    "                # 마스크 플래그가 True : 지도학습 방식으로 동작\n",
    "                # 마스크 플래그가 False : 비지도학습-자가 회귀방식으로 동작\n",
    "                input_token = torch.where(TF_flag.unsqueeze(1), \n",
    "                                            tar[:, t].unsqueeze(1),\n",
    "                                            out_tokens.argmax(dim=-1))\n",
    "\n",
    "            # 최종 출력 모양 조정 \n",
    "            outputs = torch.cat(outputs, dim=1) # (BS, tar_seq_len-1, tar_vocab)\n",
    "            # (BS, 1, tar_vocab) 차원의 sos 토큰 인덱스로 채워진 텐서를 만듬\n",
    "            sos_tokens = torch.full((BS, 1, outputs.size(-1)), en_sos, device=device)\n",
    "            # sos_tokens랑 outputs를 합쳐서 (BS, tar_seq_len, tar_vocab)가 되게 함\n",
    "            outputs = torch.cat([sos_tokens, outputs], dim=1)\n",
    "            return outputs # (BS, tar_seq_len, tar_vocab)\n",
    "        \n",
    "        else: # 정답지가 없는 평가모드\n",
    "            # 배치사이즈, 연산위치 정보 추출(src 기준으로)\n",
    "            BS, src_seq_len = src.size() \n",
    "            device = src.device\n",
    "            # 최대 디코딩 길이 지정 안했으면 원문 seq_len을 쓰자\n",
    "            if self.max_len is None:\n",
    "                self.max_len = src_seq_len\n",
    "\n",
    "            # 디코더의 첫 토큰을 <SOS>에 (BS,1)로 채우고 맨 앞에 Beam_width적용\n",
    "            input_token = torch.full((BW, BS, 1), en_sos).to(device)\n",
    "            # 디코더 첫 토큰에 대한 확률정보 생성 (BW, BS, 1)\n",
    "            pred_probs = F.softmax(input_token.type(torch.float), dim=0)\n",
    "            # 최종 출력 시퀀스는 Beam_search의 후보군 시퀀스를 다 내보내는 것으로 함\n",
    "            # 후보 시퀀스를 다 내보낸 뒤 나중에 처리하는 것으로\n",
    "            outputs = torch.full((BS, self.max_len, BW), en_pad).to(device)\n",
    "\n",
    "            # <EOS>토큰을 각 beam search, batch_size에서 예측했으면 이를 감지하는 flag\n",
    "            pad_mask_flag = torch.zeros((BW, BS), dtype=torch.bool, \n",
    "                                        device=device)\n",
    "\n",
    "            # context_vector의 차원은 (num_layers, BS, hid_dim)임\n",
    "            # 이것을 (BW, num_layers, BS, hid_dim) 4차원으로 반복 복제로 늘림\n",
    "            if isinstance(context_h, tuple): # LSTM버전인 경우\n",
    "                con_h, con_c = context_h\n",
    "                con_h = con_h.unsqueeze(0).repeat(BW, 1, 1, 1)\n",
    "                con_c = con_c.unsqueeze(0).repeat(BW, 1, 1, 1)\n",
    "                context_h = (con_h, con_c)\n",
    "            else:\n",
    "                context_h = context_h.unsqueeze(0).repeat(BW, 1, 1, 1)\n",
    "\n",
    "            # 토큰 단위로 예측이니 자가 회귀 방식임\n",
    "            h = context_h\n",
    "            for t in range(self.max_len):\n",
    "                # 디코더에 입력하여 out_tokens를 만들어야 하니 입력 가능하게\n",
    "                # BW * BS를 곱해서 3차원으로 축소\n",
    "                input_token = input_token.view(BW*BS, -1) #(BW*BS, 1)\n",
    "                if isinstance(h, tuple): # LSTM버전인 경우\n",
    "                    ele_h, ele_c = h #아래의 코드는 # (nL, BW*BS, hid_dim)로 차원변환\n",
    "                    ele_h = ele_h.view(ele_h.size(1), BW*BS, ele_h.size(3)) \n",
    "                    ele_c = ele_c.view(ele_c.size(1), BW*BS, ele_c.size(3))\n",
    "                    h = (ele_h, ele_c) #튜플로 원복\n",
    "                else:\n",
    "                    h = h.view(h.size(1), BW*BS, h.size(3)) # (nL, BW*BS, hid_dim)\n",
    "                \n",
    "                # 출력된 out_tokens의 차원은 (BW*BS, 1, tar_vocab)이다.\n",
    "                out_tokens, h = self.decode(input_token, h)\n",
    "\n",
    "                # 원래 차원인 (BW, BS ,,)순으로 모두 복원\n",
    "                out_tokens = out_tokens.view(BW, BS, 1, -1) # (BW, BS, 1, tar_vocab)\n",
    "                # h = h.view(BW, h.size(0), BS, -1)   # (BW, nL, BS, hid_dim)\n",
    "                \n",
    "                # 모델이 예측한 토큰에 대한 확률정보 생성\n",
    "                cur_probs = F.softmax(out_tokens, dim=-1) # (BW, BS, 1, tar_vocab)\n",
    "\n",
    "                # cur_probs의 가장 높은 확률을 가진 BW(K)개 데이터 val, idx 추출\n",
    "                # val, idx의 BW개를 선택한 차원정보 : (BW, BS, 1, BW)\n",
    "                top_k_probs, top_k_idxs = torch.topk(input=cur_probs, k=BW, dim=-1)\n",
    "                \n",
    "                # pred_probs의 차원을 늘려서 (BW, BS, 1, 1)로 만든 다음\n",
    "                # 브로드 캐스팅 방법으로 top_k_probs와 곱한 뒤 (BW, BS, 1, BW)\n",
    "                # reshape를 적용하여 (BW*BW, BS, 1) 차원으로 변환\n",
    "                scores = (pred_probs.unsqueeze(-1) * top_k_probs).view(BW*BW, BS, 1)\n",
    "                # sorces 정보에 대해서 beam search로 BW개만큼의 데이터, idx 추출 (BW, BS, 1)\n",
    "                _, top_sorce_idx = torch.topk(input=scores, k=BW, dim=0)\n",
    "\n",
    "                # context vector 추적 및 추적정보를 바탕으로 갱신하기\n",
    "                if isinstance(h, tuple): #LSTM버전인 경우\n",
    "                    h, c = self._devide_data(h)\n",
    "                    h = self._track_hidden(h, top_sorce_idx, BW)\n",
    "                    c = self._track_hidden(c, top_sorce_idx, BW)\n",
    "                    h = self._combine_data(h, c)\n",
    "                else:\n",
    "                    h = self._track_hidden(h, top_sorce_idx, BW)\n",
    "\n",
    "                # 이전토큰의 확률정보 pred_probs 업데이트\n",
    "                # cur_probs에서 상위 BW개를 추출한 top_k_probs (BW, BS, 1, BW)를\n",
    "                # (BW*BW, BS, 1)로 차원전환한 뒤 score의 BW개 idx 정보로 값 서치\n",
    "                # 따라서 업데이트 된 pred_probs는 (BW, BS, 1)차원으로 정상적으로 업데이트\n",
    "                pred_probs = torch.gather(top_k_probs.view(BW*BW, BS, 1), 0, top_sorce_idx)\n",
    "\n",
    "                # 입력토큰은 top_sorce_idx 정보를 바탕으로 top_k_idxs의 인덱스 정보를 찾으면 됨\n",
    "                input_token = torch.gather(top_k_idxs.view(BW*BW, BS, 1), 0, top_sorce_idx)\n",
    "\n",
    "                # out_tokes를 최종 출력물을 outputs의 t번째 seq에 덮어쓰기\n",
    "                outputs[:, t, :] = input_token.squeeze(-1).transpose(0, 1)  # (BS, max_len, BW)\n",
    "\n",
    "                # 업데이트한 input_token이 <EOS>토큰의 idx를 예측햇는지 검토\n",
    "                eos_indices = (input_token.squeeze(1) == en_eos)\n",
    "                # OR연산을 통해 pad_mask_flag가 <EOS>토큰의 idx를 예측했으면 TRUE로 전환한다\n",
    "                pad_mask_flag = pad_mask_flag | eos_indices\n",
    "\n",
    "                # 모든 샘플이 EOS예측\n",
    "                if pad_mask_flag.all():\n",
    "                    break\n",
    "            \n",
    "            return outputs # (BS, max_len, BW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실험 조건을 구분하기 위한 키\n",
    "model_key = ['LSTM', 'GRU']\n",
    "metrics_key = ['Loss', '정확도']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq의 LSTM버전 인스턴스화\n",
    "encoder_lstm = Encoder('LSTM', src_VS, EMB_DIM, UNIT_DIM, \n",
    "                            num_layer=NUM_Layers, bi=BI_DIR)\n",
    "decoder_lstm = Decoder('LSTM', tar_VS, EMB_DIM, UNIT_DIM,\n",
    "                            num_layer=NUM_Layers, bi=BI_DIR)\n",
    "Translater_lstm = Seq2Seq(encoder_lstm, decoder_lstm, \n",
    "                        spec_token=spec_token, max_len=tar_SL)\n",
    "\n",
    "# Seq2Seq의 GRU버전 인스턴스화\n",
    "encoder_gru = Encoder('GRU', src_VS, EMB_DIM, UNIT_DIM, \n",
    "                            num_layer=NUM_Layers, bi=BI_DIR)\n",
    "decoder_gru = Decoder('GRU', tar_VS, EMB_DIM, UNIT_DIM,\n",
    "                            num_layer=NUM_Layers, bi=BI_DIR)\n",
    "Translater_gru = Seq2Seq(encoder_gru, decoder_gru, \n",
    "                        spec_token=spec_token, max_len=tar_SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 저장\n",
    "path = {} #모델별 경로명 저장\n",
    "for mk in model_key:\n",
    "    path[mk] = f'Seq2Seq_TF_{mk}.pth'\n",
    "    # torch.save(models[mk].state_dict(), path[mk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 불러오기\n",
    "load_model = {\n",
    "    'LSTM': Translater_lstm,  # LSTM 모델 인스턴스 생성\n",
    "    'GRU': Translater_gru     # GRU 모델 인스턴스 생성\n",
    "}\n",
    "for mk in model_key:\n",
    "    load_model[mk].load_state_dict(torch.load(path[mk], weights_only=True))\n",
    "    #추론기는 CPU에서 돌리자\n",
    "    load_model[mk] = load_model[mk].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# 테스트 데이터셋에서 샘플을 추출\n",
    "# 전체 테스트 데이터 개수정보를 추출\n",
    "num_test = padded_src_test.shape[0]\n",
    "sample_epoch = 5 #추출할 샘플 개수 정의\n",
    "indices = random.sample(range(num_test), sample_epoch)\n",
    "\n",
    "# 추출한 샘플번호를 바탕으로 Test 데이터셋에서 무작위 추출\n",
    "S_src_test = padded_src_test[indices]\n",
    "S_tar_test = padded_tar_test[indices]\n",
    "\n",
    "# 원문 데이터만 텐서 자료형으로 변환\n",
    "TS_src_test = torch.tensor(S_src_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--현재 추론 조건: [Seq2Seq_LSTM]--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 27.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--현재 추론 조건: [Seq2Seq_GRU]--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 28.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from Seq_trainer import *\n",
    "\n",
    "# 추론 결과를 저장할 딕셔너리\n",
    "tar_dict = {key: []  for key in model_key}\n",
    "\n",
    "BW = 3 #Beam Search의 Search Space 계수(Beam_width)값\n",
    "\n",
    "for key in model_key:\n",
    "    print(f\"\\n--현재 추론 조건: [Seq2Seq_{key}]--\") # 조건에 맞는 실험시작\n",
    "    for idx in tqdm(range(sample_epoch)): #추론 에포크별 추론 시작\n",
    "        # 입력되는 원문 차원을 (1, src_seq_len)으로 만들기 위한 코드\n",
    "        iter_src_data = TS_src_test[idx].unsqueeze(0)\n",
    "        # 모델 추론 -> 추론결과는 (bs, max_len, BW) ndarray타입임\n",
    "        # BW 옵션이 1 -> 사실상 Greedy Search랑 같은 결과\n",
    "        # BW 옵션이 1 이상 -> 제대로된 Beam Search 작업수행\n",
    "        tar_infer_doc = model_inference(load_model[key], \n",
    "                                        iter_src_data, BW=BW)\n",
    "        tar_dict[key].append(tar_infer_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 번역 결과 확인\n",
      "원문(src) : dort sind er und die weiteren insassen unmenschlichen bedingungen ausgesetzt , die schließlich in einer revolte munden .\n",
      "\n",
      "seqLSTM_0_번역 : 그곳에서 그는 당시에 성장하여 , 의미 그녀는 다는 .\n",
      "seq-GRU_0_번역 : 그곳에서 그는 많은 선택권을 줍니다 . . .\n",
      "seqLSTM_1_번역 : 그곳에서 그는 빠르게 성장하여 , 미스터 사람들은 그리고 이어 명 [UNK] .죠 .\n",
      "seq-GRU_1_번역 : 그곳에서 그는 필요한 질문을 지구 있습니다 . 싸 있습니다 . .\n",
      "seqLSTM_2_번역 : 7세옵 빠르게 순한 관점에서 , 전의 , 영화합니다 ,고 , 라고 않다\n",
      "seq-GRU_2_번역 : 동물에게 필요한 사람을권이 깊숙이 끌어들이는 다가오고여 있습니다\n",
      "\n",
      "번역문(tar) : 그곳에서 그와 다른 수감자들은 비인간적인 조건에 노출되어 결국 반란을 일으킨다 .\n",
      "==============================\n",
      "\n",
      "2번째 번역 결과 확인\n",
      "원문(src) : sie sind sich sehr , sehr ahnlich .\n",
      "\n",
      "seqLSTM_0_번역 : 그들은 매우 부드럽게 , 매우 . . .\n",
      "seq-GRU_0_번역 : 그들은 매우 도시를 . 나왔습니다 . . .\n",
      "seqLSTM_1_번역 : 그들은 매우 부드럽게 유지됩니다 .입니다 .\n",
      "seq-GRU_1_번역 : 그들은 매우 갑자기 서커스가 있습니다 . . 나왔습니다됩니다 . 있습니다보기 . . 나왔습니다 나왔습니다 .\n",
      "seqLSTM_2_번역 : 불순 아주 다르고 유지됩니다 차갑습니다입니다 . 않습니다\n",
      "seq-GRU_2_번역 : 여행자는 중요합니다 바로투 . . 시작 매우 가지고보기 나왔습니다 시작합니다 나왔습니다 . . . 나왔습니다\n",
      "\n",
      "번역문(tar) : 그들은 매우 매우 유사합니다 .\n",
      "==============================\n",
      "\n",
      "3번째 번역 결과 확인\n",
      "원문(src) : cooper nimmt die verfolgung auf .\n",
      "\n",
      "seqLSTM_0_번역 : 성장에 성장에 시키글라기즙 것은은은은\n",
      "seq-GRU_0_번역 : 패 들어왔한 동의합니다 . 싸 . .니다 . 나왔습니다니다\n",
      "seqLSTM_1_번역 : 카 고려하여 성장에 성장에 의미 많이 재료가지도은는 .\n",
      "seq-GRU_1_번역 : 패사는어 만듭니다 . 나왔습니다웁니다 . . 나왔습니다 . 나왔습니다 . .\n",
      "seqLSTM_2_번역 : 카 여의폰라기 코어라기 맛이입니다고 . 명\n",
      "seq-GRU_2_번역 : 어지사는 적 때에 이산아 호기심다 .보기 싸 나왔습니다 . 싸웁 . . 싸웁\n",
      "\n",
      "번역문(tar) : 쿠퍼는 추적하고 있습니다 .\n",
      "==============================\n",
      "\n",
      "4번째 번역 결과 확인\n",
      "원문(src) : kann technik die arbeit von insekten ersetzen ?\n",
      "\n",
      "seqLSTM_0_번역 : 퍼포먼스지사 우리의 인식을는 무엇입니까 .\n",
      "seq-GRU_0_번역 : 바라니 어떤 경우에는 기후보기 . . .\n",
      "seqLSTM_1_번역 : 전면에 테니럿 미래가 있습니까 ? ? ? ?난다 . 않습니다 .습니다 .\n",
      "seq-GRU_1_번역 : 재은 바다로 열어야 변화를 . 시작합니다보기됩니다 나왔습니다 . 큽니다 전\n",
      "seqLSTM_2_번역 : 심해에서 선전 연구에는 어떻게 되었습니까 ? 않습니까 해냈습니다 .난다 .났 않습니다\n",
      "seq-GRU_2_번역 : 곤충와 당신의 만듭니다 . 합니다 어떻게 넣을 시작 . 전보기 .례\n",
      "\n",
      "번역문(tar) : 기술이 곤충의 일을 대체할 수 있습니까 ?\n",
      "==============================\n",
      "\n",
      "5번째 번역 결과 확인\n",
      "원문(src) : im mittelalter sagten die leute , dies seien schlangenzungen , die vom heiligen paulus versteinert worden waren .\n",
      "\n",
      "seqLSTM_0_번역 : 성장에 아시죠 초월년을라기 [UNK]옵 [UNK] [UNK] [UNK]\n",
      "seq-GRU_0_번역 : 일본 , 청중들은 액체로 해수면 상승을 대신할 수 있습니다 .\n",
      "seqLSTM_1_번역 : 이미 귀엽고지사피는 [UNK] 때마다 [UNK] , 그녀의 명 .\n",
      "seq-GRU_1_번역 : 일본 , 청중들은 액체로 해수면 상승을 대신할 수 있습니다 .\n",
      "seqLSTM_2_번역 : 이미옵 초월 아쉬 가격은 , 명은 , [UNK]\n",
      "seq-GRU_2_번역 : 그 식물들은소들이 휴대가 바닷물로 앞에서으로 통해하는 수도 있습니다 .\n",
      "\n",
      "번역문(tar) : 중세 시대 사람들은 , 이것이 성 바울에 의해 석화된 , 뱀의 혀라고 말했습니다 .\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx_list = range(1, sample_epoch+1)\n",
    "for idx, src, pred_lstm, pred_gru, tar in zip(idx_list, S_src_test, \n",
    "                                              tar_dict['LSTM'], \n",
    "                                              tar_dict['GRU'], \n",
    "                                              S_tar_test):\n",
    "    # 원문, 모델번역문_1, 모델번역문_2, 정답번역문 순으로 디코딩\n",
    "\n",
    "    decode_src = Translater_post_processor(src, idx_to_src, spec_token)\n",
    "    \n",
    "    de_pred_lstm_list = [] #Beam_search를 수행함으로 인한 후보군 데이터를 저장하는 리스트 \n",
    "    de_pred_gru_list = []  #Beam_search를 수행함으로 인한 후보군 데이터를 저장하는 리스트\n",
    "\n",
    "    for BW_i in range(BW):\n",
    "        de_pred_lstm = Translater_post_processor(pred_lstm[:, BW_i], idx_to_tar, spec_token)\n",
    "        de_pred_gru = Translater_post_processor(pred_gru[:, BW_i], idx_to_tar, spec_token)\n",
    "        \n",
    "        de_pred_lstm_list.append(de_pred_lstm)\n",
    "        de_pred_gru_list.append(de_pred_gru)\n",
    "\n",
    "    decode_tar = Translater_post_processor(tar, idx_to_tar, spec_token)\n",
    "\n",
    "    print(f\"{idx}번째 번역 결과 확인\")\n",
    "    print(f\"원문(src) : {decode_src}\", end='\\n\\n')\n",
    "\n",
    "    for BW_i in range(BW):\n",
    "        print(f\"seqLSTM_{BW_i}_번역 : {de_pred_lstm_list[BW_i]}\")\n",
    "        print(f\"seq-GRU_{BW_i}_번역 : {de_pred_gru_list[BW_i]}\")\n",
    "\n",
    "    print(f\"\\n번역문(tar) : {decode_tar}\")\n",
    "    print(\"==============================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.special import softmax\n",
    "\n",
    "def BLEU_Score(candidate, reference, p_list=None, ignore_idx=0):\n",
    "    if p_list is None:\n",
    "        p_list = [0.25, 0.25, 0.25, 0.25]  # 기본적으로 균등 가중치 설정\n",
    "    \n",
    "    def n_gram_precision(candidate, reference, n, ignore_idx):\n",
    "        candidate_n_grams = [tuple(candidate[i:i+n]) \n",
    "                             for i in range(len(candidate) - n + 1) \n",
    "                                if ignore_idx not in candidate[i:i+n]]\n",
    "        reference_n_grams = [tuple(reference[i:i+n]) \n",
    "                             for i in range(len(reference) - n + 1) \n",
    "                                if ignore_idx not in reference[i:i+n]]\n",
    "        \n",
    "        candidate_counter = Counter(candidate_n_grams)\n",
    "        reference_counter = Counter(reference_n_grams)\n",
    "        \n",
    "        overlap = sum(min(candidate_counter[ng], reference_counter[ng]) \n",
    "                                for ng in candidate_counter)\n",
    "        total = sum(candidate_counter.values())\n",
    "        \n",
    "        if total == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return overlap / total\n",
    "\n",
    "\n",
    "    best_bleu_score = 0\n",
    "    best_candidate = None\n",
    "    bleu_score_list = []\n",
    "\n",
    "    for i in range(candidate.shape[1]):  # BW 개수만큼 후보군을 반복\n",
    "        current_candidate = candidate[:, i]\n",
    "        precisions = []\n",
    "        \n",
    "        for n in range(1, 5):  # 1-gram, 2-gram, 3-gram, 4-gram\n",
    "            p_n = n_gram_precision(current_candidate, reference, n, ignore_idx)\n",
    "            # Smoothing: precision이 0인 경우 작은 값(예: 1e-9)으로 대체\n",
    "            precisions.append(p_n if p_n > 0 else 1e-9)\n",
    "        \n",
    "        log_precisions = [p_list[n-1] * np.log(p) for n, p in \n",
    "                            enumerate(precisions, start=1)]\n",
    "        bleu_score = np.exp(sum(log_precisions))\n",
    "\n",
    "        bleu_score_list.append(bleu_score*(10^20))\n",
    "        \n",
    "\n",
    "    # bleu_score_result의 값이 원체 작아서 softmax 처리\n",
    "    bleu_score_list = softmax(bleu_score_list)\n",
    "\n",
    "    # 가장 높은 BLEU score를 가진 후보군 선택\n",
    "    best_idx = np.argmax(bleu_score_list)\n",
    "    best_bleu_score = bleu_score_list[best_idx]\n",
    "    best_candidate = candidate[:, best_idx]\n",
    "    \n",
    "    bleu_dict = {\n",
    "        '최고BLUE점수': f'{best_bleu_score * 100:.2f}%',\n",
    "        'Best_y예측': best_candidate\n",
    "    }\n",
    "    \n",
    "    return bleu_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 번역 결과 확인\n",
      "원문(src) : dort sind er und die weiteren insassen unmenschlichen bedingungen ausgesetzt , die schließlich in einer revolte munden .\n",
      "seqLSTM_점수: 33.34%\n",
      "seqLSTM_번역: 그곳에서 그는 당시에 성장하여 , 의미 그녀는 다는 .\n",
      "seqGRU_점수: 33.34%\n",
      "seqGRU_번역: 그곳에서 그는 많은 선택권을 줍니다 . . .\n",
      "번역문(tar) : 그곳에서 그와 다른 수감자들은 비인간적인 조건에 노출되어 결국 반란을 일으킨다 .\n",
      "==============================\n",
      "\n",
      "2번째 번역 결과 확인\n",
      "원문(src) : sie sind sich sehr , sehr ahnlich .\n",
      "seqLSTM_점수: 33.34%\n",
      "seqLSTM_번역: 그들은 매우 부드럽게 , 매우 . . .\n",
      "seqGRU_점수: 33.33%\n",
      "seqGRU_번역: 그들은 매우 도시를 . 나왔습니다 . . .\n",
      "번역문(tar) : 그들은 매우 매우 유사합니다 .\n",
      "==============================\n",
      "\n",
      "3번째 번역 결과 확인\n",
      "원문(src) : cooper nimmt die verfolgung auf .\n",
      "seqLSTM_점수: 33.34%\n",
      "seqLSTM_번역: 카 고려하여 성장에 성장에 의미 많이 재료가지도은는 .\n",
      "seqGRU_점수: 33.34%\n",
      "seqGRU_번역: 패 들어왔한 동의합니다 . 싸 . .니다 . 나왔습니다니다\n",
      "번역문(tar) : 쿠퍼는 추적하고 있습니다 .\n",
      "==============================\n",
      "\n",
      "4번째 번역 결과 확인\n",
      "원문(src) : kann technik die arbeit von insekten ersetzen ?\n",
      "seqLSTM_점수: 33.34%\n",
      "seqLSTM_번역: 전면에 테니럿 미래가 있습니까 ? ? ? ?난다 . 않습니다 .습니다 .\n",
      "seqGRU_점수: 33.33%\n",
      "seqGRU_번역: 곤충와 당신의 만듭니다 . 합니다 어떻게 넣을 시작 . 전보기 .례\n",
      "번역문(tar) : 기술이 곤충의 일을 대체할 수 있습니까 ?\n",
      "==============================\n",
      "\n",
      "5번째 번역 결과 확인\n",
      "원문(src) : im mittelalter sagten die leute , dies seien schlangenzungen , die vom heiligen paulus versteinert worden waren .\n",
      "seqLSTM_점수: 33.34%\n",
      "seqLSTM_번역: 이미 귀엽고지사피는 [UNK] 때마다 [UNK] , 그녀의 명 .\n",
      "seqGRU_점수: 33.33%\n",
      "seqGRU_번역: 일본 , 청중들은 액체로 해수면 상승을 대신할 수 있습니다 .\n",
      "번역문(tar) : 중세 시대 사람들은 , 이것이 성 바울에 의해 석화된 , 뱀의 혀라고 말했습니다 .\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BLEU score를 적용하여 원문 / 예측구문 / 번역문 결과 확인\n",
    "idx_list = range(1, sample_epoch+1)\n",
    "for idx, src, pred_lstm, pred_gru, tar in zip(idx_list, S_src_test, \n",
    "                                              tar_dict['LSTM'], \n",
    "                                              tar_dict['GRU'], \n",
    "                                              S_tar_test):\n",
    "    # 원문, 모델번역문_1, 모델번역문_2, 정답번역문 순으로 디코딩\n",
    "    decode_src = Translater_post_processor(src, idx_to_src, spec_token)\n",
    "\n",
    "    # 모델 예측 결과물에 대하여 BLEU 스코어 및 Best_y_pred 추출\n",
    "    BLEU_LSTM = BLEU_Score(pred_lstm, tar)\n",
    "    BLEU_LSTM_score = BLEU_LSTM['최고BLUE점수']\n",
    "    b_de_pred_lstm = Translater_post_processor(BLEU_LSTM['Best_y예측'], idx_to_tar, spec_token)\n",
    "    BLEU_GRU = BLEU_Score(pred_gru, tar)\n",
    "    BLEU_GRU_score = BLEU_GRU['최고BLUE점수']\n",
    "    b_de_pred_gru = Translater_post_processor(BLEU_GRU['Best_y예측'], idx_to_tar, spec_token)\n",
    "\n",
    "    decode_tar = Translater_post_processor(tar, idx_to_tar, spec_token)\n",
    "\n",
    "    print(f\"{idx}번째 번역 결과 확인\")\n",
    "    print(f\"원문(src) : {decode_src}\", end='\\n')\n",
    "\n",
    "    print(f\"seqLSTM_점수: {BLEU_LSTM_score}\\nseqLSTM_번역: {b_de_pred_lstm}\")\n",
    "    print(f\"seqGRU_점수: {BLEU_GRU_score}\\nseqGRU_번역: {b_de_pred_gru}\")\n",
    "\n",
    "    print(f\"번역문(tar) : {decode_tar}\")\n",
    "    print(\"==============================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
