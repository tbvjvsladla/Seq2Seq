{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from NLP_pp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문서ID</th>\n",
       "      <th>번역</th>\n",
       "      <th>원문(src)</th>\n",
       "      <th>번역문(tar)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ot_100615</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>mal sehen , ob meine haut reagiert oder nicht .</td>\n",
       "      <td>내 피부가 반응하는지 보자 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ot_100616</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>ich habe viele großartige kritiken über drunk ...</td>\n",
       "      <td>저는 드렁크 엘리펀트에 대한 훌륭한 리뷰를 많이 들었습니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ot_100617</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>lass uns zur poolparty gehen .</td>\n",
       "      <td>수영장 파티에 가자 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ot_100618</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>sie bringen einen neuen reiniger auf den markt .</td>\n",
       "      <td>그들은 새로운 청소기를 시장에서 출시하고 있어요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ot_100619</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>genau hier ist nur gepresster gemüsesaft .</td>\n",
       "      <td>바로 여기에 압착 야채 주스가 있어요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120327</th>\n",
       "      <td>md_100608</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>er sucht nach dem ort , der die bewohner verei...</td>\n",
       "      <td>그는 주민들을 결집시킬 수 있는 장소를 찾아다녔고 , 이를 빵집에서 찾는다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120328</th>\n",
       "      <td>md_100609</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>coelho costinha ist ein einfacher mann , der j...</td>\n",
       "      <td>코엘호 코스틴하는 재정적인 어려움을 겪고 있는 평범한 사람이다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120329</th>\n",
       "      <td>md_100610</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>als sie aufwacht , ist sie doppelt so alt , de...</td>\n",
       "      <td>그녀가 깨어났을 때 , 그녀는 나이가 두 배나 많이 들었지만 , 여전히 10대처럼 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120330</th>\n",
       "      <td>md_100611</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>viele frauen stecken in der misere , und alle ...</td>\n",
       "      <td>많은 여성들이 불행 속에 살고 있고 , 그들 모두는 그들의 상황에서 벗어날 길을 찾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120331</th>\n",
       "      <td>md_100614</td>\n",
       "      <td>독-&gt;한</td>\n",
       "      <td>eines tages erfährt corvaz , dass sich seine m...</td>\n",
       "      <td>어느 날 , 코바즈는 그의 어머니가 자살했다는 것을 알게 된다 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120332 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             문서ID    번역                                            원문(src)  \\\n",
       "0       ot_100615  독->한    mal sehen , ob meine haut reagiert oder nicht .   \n",
       "1       ot_100616  독->한  ich habe viele großartige kritiken über drunk ...   \n",
       "2       ot_100617  독->한                     lass uns zur poolparty gehen .   \n",
       "3       ot_100618  독->한   sie bringen einen neuen reiniger auf den markt .   \n",
       "4       ot_100619  독->한         genau hier ist nur gepresster gemüsesaft .   \n",
       "...           ...   ...                                                ...   \n",
       "120327  md_100608  독->한  er sucht nach dem ort , der die bewohner verei...   \n",
       "120328  md_100609  독->한  coelho costinha ist ein einfacher mann , der j...   \n",
       "120329  md_100610  독->한  als sie aufwacht , ist sie doppelt so alt , de...   \n",
       "120330  md_100611  독->한  viele frauen stecken in der misere , und alle ...   \n",
       "120331  md_100614  독->한  eines tages erfährt corvaz , dass sich seine m...   \n",
       "\n",
       "                                                 번역문(tar)  \n",
       "0                                        내 피부가 반응하는지 보자 .  \n",
       "1                      저는 드렁크 엘리펀트에 대한 훌륭한 리뷰를 많이 들었습니다 .  \n",
       "2                                            수영장 파티에 가자 .  \n",
       "3                            그들은 새로운 청소기를 시장에서 출시하고 있어요 .  \n",
       "4                                  바로 여기에 압착 야채 주스가 있어요 .  \n",
       "...                                                   ...  \n",
       "120327        그는 주민들을 결집시킬 수 있는 장소를 찾아다녔고 , 이를 빵집에서 찾는다 .  \n",
       "120328               코엘호 코스틴하는 재정적인 어려움을 겪고 있는 평범한 사람이다 .  \n",
       "120329  그녀가 깨어났을 때 , 그녀는 나이가 두 배나 많이 들었지만 , 여전히 10대처럼 ...  \n",
       "120330  많은 여성들이 불행 속에 살고 있고 , 그들 모두는 그들의 상황에서 벗어날 길을 찾...  \n",
       "120331               어느 날 , 코바즈는 그의 어머니가 자살했다는 것을 알게 된다 .  \n",
       "\n",
       "[120332 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('exam_deko.csv')\n",
    "\n",
    "display(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더의 입력 데이터인 src 데이터\n",
    "raw_src_data = raw_data['원문(src)'].values.tolist()\n",
    "# 디코더의 정답지 데이터인 tar 데이터\n",
    "raw_tar_data = raw_data['번역문(tar)'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 독일어 토크나이저 모델: dbmdz/bert-base-german-uncased\n",
    "german_tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\")\n",
    "# 한국어 토크나이저 모델: kykim/bert-kor-base\n",
    "korean_tokenizer = BertTokenizer.from_pretrained(\"kykim/bert-kor-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "토큰화 진행 중: 100%|██████████| 120332/120332 [00:24<00:00, 4957.49it/s]\n",
      "토큰화 진행 중: 100%|██████████| 120332/120332 [00:14<00:00, 8417.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# 원문 토큰화 수행\n",
    "tokenized_src_data = tokenize(raw_src_data, german_tokenizer, arch='Bert')\n",
    "# 번역문 토큰화 수행\n",
    "tokenized_tar_data = tokenize(raw_tar_data, korean_tokenizer, arch='Bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련/검증/평가를 80%, 15%, 5%로 분할을 수행\n",
    "# random_state -> 데이터셋을 내누는데 '재현성' 유지를 위해 넣음 -> 안넣어도 됨\n",
    "# stratify -> y 클래스 비율을 알기 어렵기에 해당 항목은 없앰\n",
    "src_train, src_etc, tar_train, tar_etc = train_test_split(\n",
    "    tokenized_src_data, tokenized_tar_data, test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 그 외 데이터셋을 반반으로 Val, Test로 나눔\n",
    "src_val, src_test, tar_val, tar_test = train_test_split(\n",
    "    src_etc, tar_etc, test_size=0.25,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "src_word_list = []\n",
    "tar_word_list = []\n",
    "# train항목을 워드 리스트에 입력\n",
    "for src_sent, tar_sent in zip(src_train, tar_train):\n",
    "    for word in src_sent:\n",
    "        src_word_list.append(word)\n",
    "    for word in tar_sent:\n",
    "        tar_word_list.append(word)\n",
    "# val항목을 워드 리스트에 입력\n",
    "for src_sent, tar_sent in zip(src_train, tar_train):\n",
    "    for word in src_sent:\n",
    "        src_word_list.append(word)\n",
    "    for word in tar_sent:\n",
    "        tar_word_list.append(word)\n",
    "\n",
    "# 단어와 해당 단어의 출몰 빈도를 함께 저장하는\n",
    "# Counter 타입의 변수 생성\n",
    "src_word_counts = Counter(src_word_list)\n",
    "tar_word_counts = Counter(tar_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---원문(src)에 대한 희소단어 분석---\n",
      "\n",
      "---번역문(tar)에 대한 희소단어 분석---\n"
     ]
    }
   ],
   "source": [
    "rare_th = 0 #희소단어의 등장 빈도를 결정하는 파라미터\n",
    "# 희소단어 등장 빈도를 바탕으로 희소 단어를 배제하기 위해 준비 함수\n",
    "print(f'---원문(src)에 대한 희소단어 분석---')\n",
    "src_tot_vocab_cnt, src_rare_vocab_cnt = set_rare_vocab(src_word_counts, rare_th)\n",
    "print(f'\\n---번역문(tar)에 대한 희소단어 분석---')\n",
    "tar_tot_vocab_cnt, tar_rare_vocab_cnt = set_rare_vocab(tar_word_counts, rare_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#등장 빈도가 높은 단어 순으로 정렬하기\n",
    "src_vocab = sorted(src_word_counts, key=src_word_counts.get, reverse=True)\n",
    "tar_vocab = sorted(tar_word_counts, key=tar_word_counts.get, reverse=True)\n",
    "\n",
    "# 원문(src)에 대한 희소단어 배제 & 정렬 작업 수행\n",
    "src_vocab_size = src_tot_vocab_cnt - src_rare_vocab_cnt\n",
    "src_vocab = src_vocab[:src_vocab_size]\n",
    "# 번역문(tar에 대한 희소단어 배제 & 정렬 작업 수행\n",
    "tar_vocab_size = tar_tot_vocab_cnt - tar_rare_vocab_cnt\n",
    "tar_vocab = tar_vocab[:tar_vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---원문(src)에 대한 단어장 분석---\n",
      "단어집합(vocab)은 word_to_idx를 통해서\n",
      "[단어 : idx]의 <class 'dict'>타입이 되고\n",
      "스페셜 토큰 <PAD> <UNK> <SOS> <EOS> 을 포함하여\n",
      "총 관리되는 단어 '22471' -> '22475'가 됨\n",
      "\n",
      "---번역문(tar)에 대한 단어장 분석---\n",
      "단어집합(vocab)은 word_to_idx를 통해서\n",
      "[단어 : idx]의 <class 'dict'>타입이 되고\n",
      "스페셜 토큰 <PAD> <UNK> <SOS> <EOS> 을 포함하여\n",
      "총 관리되는 단어 '25697' -> '25701'가 됨\n"
     ]
    }
   ],
   "source": [
    "# 스페셜 토큰 선언\n",
    "spec_token = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "# 스페셜 토큰을 포함한 {단어:단어idx}의 딕셔너리 생성\n",
    "print(f'---원문(src)에 대한 단어장 분석---')\n",
    "src_to_idx, idx_to_src = set_word_to_idx(spec_token, src_vocab, \n",
    "                                         report=True)\n",
    "print(f'\\n---번역문(tar)에 대한 단어장 분석---')\n",
    "tar_to_idx, idx_to_tar = set_word_to_idx(spec_token, tar_vocab, \n",
    "                                         report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문(src) 데이터셋의 정수 인코딩 수행\n",
    "e_src_train = text_to_sequences(src_train, src_to_idx)\n",
    "e_src_val = text_to_sequences(src_val, src_to_idx)\n",
    "e_src_test = text_to_sequences(src_test, src_to_idx)\n",
    "\n",
    "# 번역문(tar) 데이터셋의 정수 인코딩 수행\n",
    "e_tar_train = text_to_sequences(tar_train, tar_to_idx)\n",
    "e_tar_val = text_to_sequences(tar_val, tar_to_idx)\n",
    "e_tar_test = text_to_sequences(tar_test, tar_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역문(tar)의 접두/접미에 SOS, EOS 토큰 추가\n",
    "e_tar_train = prefix_suffix_token_insert(e_tar_train, spec_token)\n",
    "e_tar_val = prefix_suffix_token_insert(e_tar_val, spec_token)\n",
    "e_tar_test = prefix_suffix_token_insert(e_tar_test, spec_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 문장 길이가 60 이하 데이터 비율: 100.00%\n",
      "데이터셋 문장 길이가 55 이하 데이터 비율: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 원문(src)에 대한 문장패딩 하이퍼 파라미터 설정\n",
    "src_seq_len = 60\n",
    "set_sent_pad(e_src_train, src_seq_len)\n",
    "\n",
    "# 번역문(tar)에 대한 문장패딩 하이퍼 파라미터 설정\n",
    "tar_seq_len = 55\n",
    "set_sent_pad(e_tar_train, tar_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문(src)의 문장 패딩(정수인코딩 완료)\n",
    "padded_src_train = pad_seq_x(e_src_train, src_seq_len)\n",
    "padded_src_val = pad_seq_x(e_src_val, src_seq_len)\n",
    "padded_src_test = pad_seq_x(e_src_test, src_seq_len)\n",
    "\n",
    "# 번역문(tar)의 문장 패딩(정수인코딩 완료)\n",
    "padded_tar_train = pad_seq_x(e_tar_train, tar_seq_len)\n",
    "padded_tar_val = pad_seq_x(e_tar_val, tar_seq_len)\n",
    "padded_tar_test = pad_seq_x(e_tar_test, tar_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---원문(src) 문장 패딩 결과---\n",
      "훈련용 정수(원핫)인코딩 shape: (96265, 60)\n",
      "검증용 정수(원핫)인코딩 shape: (18050, 60)\n",
      "평가용 정수(원핫)인코딩 shape: (6017, 60)\n",
      "\n",
      "---번역문(tar) 문장 패딩 결과---\n",
      "훈련용 정수(원핫)인코딩 shape: (96265, 55)\n",
      "검증용 정수(원핫)인코딩 shape: (18050, 55)\n",
      "평가용 정수(원핫)인코딩 shape: (6017, 55)\n"
     ]
    }
   ],
   "source": [
    "print(f'---원문(src) 문장 패딩 결과---')\n",
    "val_pad_shape(padded_src_train, '훈련')\n",
    "val_pad_shape(padded_src_val, '검증')\n",
    "val_pad_shape(padded_src_test, '평가')\n",
    "print(f'\\n---번역문(tar) 문장 패딩 결과---')\n",
    "val_pad_shape(padded_tar_train, '훈련')\n",
    "val_pad_shape(padded_tar_val, '검증')\n",
    "val_pad_shape(padded_tar_test, '평가')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 X(인코딩)데이터 크기: [96265, 60]\n",
      "훈련용 Y(Label)데이터 크기: [96265, 55]\n",
      "검증용 X(인코딩)데이터 크기: [18050, 60]\n",
      "검증용 Y(Label)데이터 크기: [18050, 55]\n",
      "평가용 X(인코딩)데이터 크기: [6017, 60]\n",
      "평가용 Y(Label)데이터 크기: [6017, 55]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "bs = 256 # Batch_size 하이퍼 파라미터\n",
    "\n",
    "# 정수(원핫)인코딩 데이터를 데이터로더로 변환\n",
    "trainloader = set_dataloader(padded_src_train, padded_tar_train, bs, \n",
    "                             content='훈련', report=True)\n",
    "valloader = set_dataloader(padded_src_val, padded_tar_val, bs,\n",
    "                           content='검증', report=True)\n",
    "testloader = set_dataloader(padded_src_test, padded_tar_test, bs, \n",
    "                            content='평가', report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 하이퍼 파라미터 정리\n",
    "src_VS = len(src_to_idx) # 원문 단어장 개수\n",
    "tar_VS = len(tar_to_idx) # 번역문 단어장 개수\n",
    "src_SL = src_seq_len # 원문의 문장 길이\n",
    "# 번역문의 문장 길이를 디코더(생성)문장 길이로 쓰자\n",
    "tar_SL = tar_seq_len # 번역문 문장 길이\n",
    "\n",
    "EMB_DIM = 256 # 인코더/디코더의 임베딩 레이어 차원\n",
    "# unit_dim은 인코더-디코더 사이의 히든레이어처럼 생각하는게 편함\n",
    "UNIT_DIM = 256 # 인코더와 디코더의 rnn_out 차원값\n",
    "\n",
    "NUM_Layers = 2 # 인코더/디코더의 셀은 2층으로 만들자\n",
    "BI_DIR = False # 단방향으로만 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------+\n",
      "| 하이퍼 파라미터         | 값          |\n",
      "+=========================+=============+\n",
      "| 원문 단어장 개수        | 22475개     |\n",
      "+-------------------------+-------------+\n",
      "| 번역문 단어장 개수      | 25701개     |\n",
      "+-------------------------+-------------+\n",
      "| 원문 문장 길이          | 60토큰      |\n",
      "+-------------------------+-------------+\n",
      "| 번역문 문장 길이        | 55토큰      |\n",
      "+-------------------------+-------------+\n",
      "| 원문/번역문 임베딩 차원 | 256         |\n",
      "+-------------------------+-------------+\n",
      "| 인코더-디코더 연결 차원 | 256         |\n",
      "+-------------------------+-------------+\n",
      "| 셀 층 개수              | 2층         |\n",
      "+-------------------------+-------------+\n",
      "| 양방향/단방향           | 단방향 학습 |\n",
      "+-------------------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# 출력할 데이터를 리스트 형식으로 준비\n",
    "data = [\n",
    "    [\"원문 단어장 개수\", f\"{src_VS}개\"],\n",
    "    [\"번역문 단어장 개수\", f\"{tar_VS}개\"],\n",
    "    [\"원문 문장 길이\", f\"{src_SL}토큰\"],\n",
    "    [\"번역문 문장 길이\", f\"{tar_SL}토큰\"],\n",
    "    [\"원문/번역문 임베딩 차원\", EMB_DIM],\n",
    "    [\"인코더-디코더 연결 차원\", UNIT_DIM],\n",
    "    [\"셀 층 개수\", f\"{NUM_Layers}층\"],\n",
    "    [\"양방향/단방향\", \"단방향 학습\"],\n",
    "]\n",
    "\n",
    "# 표 형식으로 출력\n",
    "print(tabulate(data, tablefmt=\"grid\",\n",
    "        headers=[\"하이퍼 파라미터\", \"값\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder_LSTM(nn.Module):\n",
    "    def __init__(self, src_vocab, src_emb_dim, rnn_dim, \n",
    "                 num_layer=1, bi=False):\n",
    "        super(Encoder_LSTM, self).__init__()\n",
    "        # <PAD>토큰의 인덱싱을 지정하면 해당 idx(0)은\n",
    "        # word_vector을 만들 때 모두 0으로 채워지게 만들어준다.\n",
    "        self.embed = nn.Embedding(src_vocab, src_emb_dim,\n",
    "                                  padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=src_emb_dim,#언어모델 입력차원\n",
    "                            hidden_size=rnn_dim,   #언어모델 출력차원\n",
    "                            num_layers=num_layer,  #언어모델 몇층?\n",
    "                            bidirectional=bi,      #양방향학습 On?\n",
    "                            batch_first=True)      #왠만하면 True\n",
    "    \n",
    "    def forward(self, x): # x의 차원 : (BS, src_seq_len)\n",
    "        emb = self.embed(x) # (BS, src_seq_len, src_emb_dim)\n",
    "        # 인코더에 양방향 학습을 적용한다\n",
    "        # rnn_out : (bs, src_seq_len, hidden_dim * 2)\n",
    "        # hidden : (num_layer * 2, bs, hidden_dim)\n",
    "        rnn_out , (hidden, cell) = self.lstm(emb)\n",
    "        #인코더의 출력은 context_vector\n",
    "        return hidden, cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Decoder_LSTM(nn.Module):\n",
    "    def __init__(self, tar_vocab, tar_emb_dim, rnn_dim,\n",
    "                 num_layer=1, bi=False):\n",
    "        super(Decoder_LSTM, self).__init__()\n",
    "        # <PAD>토큰의 인덱싱을 지정하면 해당 idx(0)은\n",
    "        # word_vector을 만들 때 모두 0으로 채워지게 만들어준다.\n",
    "        self.embed = nn.Embedding(tar_vocab, tar_emb_dim,\n",
    "                                  padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=tar_emb_dim,#언어모델 입력차원\n",
    "                            hidden_size=rnn_dim,   #언어모델 출력차원\n",
    "                            num_layers=num_layer,  #언어모델 몇층?\n",
    "                            bidirectional=bi,      #양방향학습 On?\n",
    "                            batch_first=True)      #왠만하면 True\n",
    "        # 디코더의 출력은 정답(번역문)의 seq_len이 되게 해야함\n",
    "        # 맞춰야 하는 클래스 개수는 정답지의 단어 개수임\n",
    "        if bi : #양방향으로 학습시에는 FC 레이어 입력차원이 두배\n",
    "            self.fc = nn.Linear(rnn_dim*2, tar_vocab)\n",
    "        else:\n",
    "            self.fc = nn.Linear(rnn_dim, tar_vocab)\n",
    "    \n",
    "    # 디코더는 인코더의 context_vector을 초기 hidden으로 입력받는다.\n",
    "    def forward(self, x, hidden, cell): # x의 차원 : (BS, tar_seq_len)\n",
    "        emb = self.embed(x) # (BS, tar_seq_len, tar_emb_dim)\n",
    "        # 디코더에 양방향 학습을 적용한다\n",
    "        # rnn_out : (bs, tar_seq_len, hidden_dim * 2)\n",
    "        # hidden : (num_layer * 2, bs, hidden_dim)\n",
    "        rnn_out, (hidden, cell) = self.lstm(emb, (hidden,cell))\n",
    "        \n",
    "        output = self.fc(rnn_out)\n",
    "        # 최종 출력은 (bs, seq_len, tar_vocab)\n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder_GRU(nn.Module):\n",
    "    def __init__(self, src_vocab, src_emb_dim, rnn_dim, \n",
    "                 num_layer=1, bi=False):\n",
    "        super(Encoder_GRU, self).__init__()\n",
    "        # <PAD>토큰의 인덱싱을 지정하면 해당 idx(0)은\n",
    "        # word_vector을 만들 때 모두 0으로 채워지게 만들어준다.\n",
    "        self.embed = nn.Embedding(src_vocab, src_emb_dim,\n",
    "                                  padding_idx=0)\n",
    "        self.gru = nn.GRU(input_size=src_emb_dim,#언어모델 입력차원\n",
    "                            hidden_size=rnn_dim,   #언어모델 출력차원\n",
    "                            num_layers=num_layer,  #언어모델 몇층?\n",
    "                            bidirectional=bi,      #양방향학습 On?\n",
    "                            batch_first=True)      #왠만하면 True\n",
    "    \n",
    "    def forward(self, x): # x의 차원 : (BS, src_seq_len)\n",
    "        emb = self.embed(x) # (BS, src_seq_len, src_emb_dim)\n",
    "        # 인코더에 양방향 학습을 적용한다\n",
    "        # rnn_out : (bs, src_seq_len, hidden_dim * 2)\n",
    "        # hidden : (num_layer * 2, bs, hidden_dim)\n",
    "        rnn_out , hidden = self.gru(emb)\n",
    "        #인코더의 출력은 context_vector\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Decoder_GRU(nn.Module):\n",
    "    def __init__(self, tar_vocab, tar_emb_dim, rnn_dim,\n",
    "                 num_layer=1, bi=False):\n",
    "        super(Decoder_GRU, self).__init__()\n",
    "        # <PAD>토큰의 인덱싱을 지정하면 해당 idx(0)은\n",
    "        # word_vector을 만들 때 모두 0으로 채워지게 만들어준다.\n",
    "        self.embed = nn.Embedding(tar_vocab, tar_emb_dim,\n",
    "                                  padding_idx=0)\n",
    "        self.gru = nn.GRU(input_size=tar_emb_dim,#언어모델 입력차원\n",
    "                            hidden_size=rnn_dim,   #언어모델 출력차원\n",
    "                            num_layers=num_layer,  #언어모델 몇층?\n",
    "                            bidirectional=bi,      #양방향학습 On?\n",
    "                            batch_first=True)      #왠만하면 True\n",
    "        # 디코더의 출력은 정답(번역문)의 seq_len이 되게 해야함\n",
    "        # 맞춰야 하는 클래스 개수는 정답지의 단어 개수임\n",
    "        if bi : #양방향으로 학습시에는 FC 레이어 입력차원이 두배\n",
    "            self.fc = nn.Linear(rnn_dim*2, tar_vocab)\n",
    "        else:\n",
    "            self.fc = nn.Linear(rnn_dim, tar_vocab)\n",
    "    \n",
    "    # 디코더는 인코더의 context_vector을 초기 hidden으로 입력받는다.\n",
    "    def forward(self, x, hidden): # x의 차원 : (BS, tar_seq_len)\n",
    "        emb = self.embed(x) # (BS, tar_seq_len, tar_emb_dim)\n",
    "        # 디코더에 양방향 학습을 적용한다\n",
    "        # rnn_out : (bs, tar_seq_len, hidden_dim * 2)\n",
    "        # hidden : (num_layer * 2, bs, hidden_dim)\n",
    "        rnn_out, hidden = self.gru(emb, hidden)\n",
    "        \n",
    "        output = self.fc(rnn_out)\n",
    "        # 최종 출력은 (bs, seq_len, tar_vocab)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Seq2Seq_LSTM(nn.Module):\n",
    "    def __init__(self, encoder, decoder, spec_token, max_len=None):\n",
    "        super(Seq2Seq_LSTM, self).__init__()\n",
    "        self.encode = encoder\n",
    "        self.decode = decoder\n",
    "        # 최대 디코딩 길이 설정\n",
    "        self.max_len = max_len\n",
    "        # 스페셜 토큰을 초기화에 입력하게 변경\n",
    "        self.spec_token = spec_token\n",
    "\n",
    "    def forward(self, src, tar=None, TF_ratio=1, BW=1):\n",
    "        # 인코더의 출력 = context_vector\n",
    "        context_h, context_c = self.encode(src)\n",
    "\n",
    "        # 스페셜 토큰에서 SOS, EOS, PAD의 정수인코딩값 추출\n",
    "        en_sos = self.spec_token.index('<SOS>')\n",
    "        en_eos = self.spec_token.index('<EOS>')\n",
    "        en_pad = self.spec_token.index('<PAD>')\n",
    "\n",
    "        if tar is not None:\n",
    "            # 배치사이즈, 연산위치 정보 추출(tar 기준으로)\n",
    "            BS, tar_seq_len = tar.size() \n",
    "            device = tar.device\n",
    "\n",
    "            # 디코더의 첫번째 토큰을 <SOS>에 (BS, 1)차원으로 생성\n",
    "            # 이때 정답지(tar)의 맨 앞토큰은 <SOS>로 채워져 있으니 이를 이용한다.\n",
    "            input_token = tar[:, 0].unsqueeze(1)\n",
    "            outputs = [] # 출력 디코드 결과를 저장\n",
    "\n",
    "            # 임의 난수를 (BS)차원으로 생성 후 TF_ratio비율정보를 받아서\n",
    "            # 마스크 플래그로 변환, 이때 TF_ratio는 0~1 사이값\n",
    "            # 1에 가까울수록 대부분의 Flag는 True가 되서 지도학습비율이 올라감\n",
    "            TF_flag = torch.rand(BS, device=device) < TF_ratio\n",
    "\n",
    "            # 토큰 단위로 예측이니 자가 회귀 방식임\n",
    "            h, c = context_h, context_c\n",
    "\n",
    "            # tar seq는 맨 처음 토큰을 <SOS>로 채웟으니 1번부터 시작\n",
    "            for t in range(1, tar_seq_len):\n",
    "                # 토큰단위로 입력이니 출력은 (bs, 1, tar_vocab)이다.\n",
    "                out_tokens, h, c = self.decode(input_token, h, c)\n",
    "                outputs.append(out_tokens)\n",
    "\n",
    "                # 마스크 플래그가 True : 지도학습 방식으로 동작\n",
    "                # 마스크 플래그가 False : 비지도학습-자가 회귀방식으로 동작\n",
    "                input_token = torch.where(TF_flag.unsqueeze(1), \n",
    "                                            tar[:, t].unsqueeze(1),\n",
    "                                            out_tokens.argmax(dim=-1))\n",
    "\n",
    "            # 최종 출력 모양 조정 \n",
    "            outputs = torch.cat(outputs, dim=1) # (BS, tar_seq_len-1, tar_vocab)\n",
    "            # (BS, 1, tar_vocab) 차원의 sos 토큰 인덱스로 채워진 텐서를 만듬\n",
    "            sos_tokens = torch.full((BS, 1, outputs.size(-1)), en_sos, device=device)\n",
    "            # sos_tokens랑 outputs를 합쳐서 (BS, tar_seq_len, tar_vocab)가 되게 함\n",
    "            outputs = torch.cat([sos_tokens, outputs], dim=1)\n",
    "            return outputs # (BS, tar_seq_len, tar_vocab)\n",
    "        \n",
    "        else: # 정답지가 없는 평가모드\n",
    "            # 배치사이즈, 연산위치 정보 추출(src 기준으로)\n",
    "            BS, src_seq_len = src.size() \n",
    "            device = src.device\n",
    "            # 최대 디코딩 길이 지정 안했으면 원문 seq_len을 쓰자\n",
    "            if self.max_len is None:\n",
    "                self.max_len = src_seq_len\n",
    "\n",
    "            # 디코더의 첫 토큰을 <SOS>에 (BS,1)로 채우고 맨 앞에 Beam_width적용\n",
    "            input_token = torch.full((BW, BS, 1), en_sos).to(device)\n",
    "            # 디코더 첫 토큰에 대한 확률정보 생성 (BW, BS, 1)\n",
    "            pred_probs = F.softmax(input_token.type(torch.float), dim=0)\n",
    "            # 최종 출력 시퀀스는 Beam_search의 후보군 시퀀스를 다 내보내는 것으로 함\n",
    "            # 후보 시퀀스를 다 내보낸 뒤 나중에 처리하는 것으로\n",
    "            outputs = torch.full((BS, self.max_len, BW), en_pad).to(device)\n",
    "\n",
    "            # <EOS>토큰을 각 beam search, batch_size에서 예측했으면 이를 감지하는 flag\n",
    "            pad_mask_flag = torch.zeros((BW, BS), dtype=torch.bool, \n",
    "                                        device=device)\n",
    "\n",
    "            # context_vector의 차원은 (num_layers, BS, hid_dim)임\n",
    "            # 이것을 (BW, num_layers, BS, hid_dim) 4차원으로 반복복제로 늘림\n",
    "            context_h = context_h.unsqueeze(0).repeat(BW, 1, 1, 1)\n",
    "            context_c = context_c.unsqueeze(0).repeat(BW, 1, 1, 1) \n",
    "\n",
    "            # 토큰 단위로 예측이니 자가 회귀 방식임\n",
    "            h, c = context_h, context_c\n",
    "\n",
    "            for t in range(self.max_len):\n",
    "                # 디코더에 입력하여 out_tokens를 만들어야 하니 입력 가능하게\n",
    "                # BW * BS를 곱해서 3차원으로 축소\n",
    "                input_token = input_token.view(BW*BS, -1) #(BW*BS, 1)\n",
    "                h = h.view(h.size(1), BW*BS, h.size(3)) # (nL, BW*BS, hid_dim)\n",
    "                c = c.view(c.size(1), BW*BS, c.size(3)) # (nL, BW*BS, hid_dim)\n",
    "                \n",
    "                # 출력된 out_tokens의 차원은 (BW*BS, 1, tar_vocab)이다.\n",
    "                out_tokens, h, c = self.decode(input_token, h, c)\n",
    "\n",
    "                # 원래 차원인 (BW, BS ,,)순으로 모두 복원\n",
    "                out_tokens = out_tokens.view(BW, BS, 1, -1) # (BW, BS, 1, tar_vocab)\n",
    "                # h = h.view(BW, h.size(0), BS, -1)   # (BW, nL, BS, hid_dim)\n",
    "                # c = c.view(BW, c.size(0), BS, -1)   # (BW, nL, BS, hid_dim)\n",
    "                \n",
    "                # 모델이 예측한 토큰에 대한 확률정보 생성\n",
    "                cur_probs = F.softmax(out_tokens, dim=-1) # (BW, BS, 1, tar_vocab)\n",
    "\n",
    "                # cur_probs의 가장 높은 확률을 가진 BW(K)개 데이터 val, idx 추출\n",
    "                # val, idx의 BW개를 선택한 차원정보 : (BW, BS, 1, BW)\n",
    "                top_k_probs, top_k_idxs = torch.topk(input=cur_probs, k=BW, dim=-1)\n",
    "                \n",
    "                # pred_probs의 차원을 늘려서 (BW, BS, 1, 1)로 만든 다음\n",
    "                # 브로드 캐스팅 방법으로 top_k_probs와 곱한 뒤 (BW, BS, 1, BW)\n",
    "                # reshape를 적용하여 (BW*BW, BS, 1) 차원으로 변환\n",
    "                scores = (pred_probs.unsqueeze(-1) * top_k_probs).view(BW*BW, BS, 1)\n",
    "                # sorces 정보에 대해서 beam search로 BW개만큼의 데이터, idx 추출 (BW, BS, 1)\n",
    "                _, top_sorce_idx = torch.topk(input=scores, k=BW, dim=0)\n",
    "\n",
    "                # context vector 추적 및 추적정보를 바탕으로 갱신하기\n",
    "                re_h = h.permute(1, 0, 2) # (BW * BS, nL, hid_dim)\n",
    "                re_c = c.permute(1, 0, 2) # (BW * BS, nL, hid_dim)\n",
    "                # (BW, BS, 1) -> (BW*BS, 1, 1) -> (BW*BS, nL, hid_dim)\n",
    "                track_h_idx = (top_sorce_idx // BW).reshape(-1, 1, 1).expand(-1, re_h.size(1), re_h.size(2))\n",
    "                track_c_idx = (top_sorce_idx // BW).reshape(-1, 1, 1).expand(-1, re_c.size(1), re_c.size(2))\n",
    "\n",
    "                re_h = torch.gather(re_h, 0, track_h_idx) # (BW*BS, nL, hid_dim)\n",
    "                re_c = torch.gather(re_c, 0, track_c_idx) # (BW*BS, nL, hid_dim)\n",
    "                # (BW*BS, nL, hid_dim) -> (nL, BW*BS, hid_dim) -> (BW, nL, BS, hid_dim)\n",
    "                h = re_h.permute(1, 0, 2).reshape(BW, h.size(0), BS, h.size(2))\n",
    "                c = re_c.permute(1, 0, 2).reshape(BW, c.size(0), BS, c.size(2))\n",
    "\n",
    "                # 이전토큰의 확률정보 pred_probs 업데이트\n",
    "                # cur_probs에서 상위 BW개를 추출한 top_k_probs (BW, BS, 1, BW)를\n",
    "                # (BW*BW, BS, 1)로 차원전환한 뒤 score의 BW개 idx 정보로 값 서치\n",
    "                # 따라서 업데이트 된 pred_probs는 (BW, BS, 1)차원으로 정상적으로 업데이트\n",
    "                pred_probs = torch.gather(top_k_probs.view(BW*BW, BS, 1), 0, top_sorce_idx)\n",
    "\n",
    "                # 입력토큰은 top_sorce_idx 정보를 바탕으로 top_k_idxs의 인덱스 정보를 찾으면 됨\n",
    "                input_token = torch.gather(top_k_idxs.view(BW*BW, BS, 1), 0, top_sorce_idx)\n",
    "\n",
    "                # out_tokes를 최종 출력물을 outputs의 t번째 seq에 덮어쓰기\n",
    "                outputs[:, t, :] = input_token.squeeze(-1).transpose(0, 1)  # (BS, max_len, BW)\n",
    "\n",
    "                # 업데이트한 input_token이 <EOS>토큰의 idx를 예측햇는지 검토\n",
    "                eos_indices = (input_token.squeeze(1) == en_eos)\n",
    "                # OR연산을 통해 pad_mask_flag가 <EOS>토큰의 idx를 예측했으면 TRUE로 전환한다\n",
    "                pad_mask_flag = pad_mask_flag | eos_indices\n",
    "\n",
    "                # 모든 샘플이 EOS예측\n",
    "                if pad_mask_flag.all():\n",
    "                    break\n",
    "            \n",
    "            return outputs # (BS, max_len, BW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Seq2Seq_GRU(nn.Module):\n",
    "    def __init__(self, encoder, decoder, spec_token, max_len=None):\n",
    "        super(Seq2Seq_GRU, self).__init__()\n",
    "        self.encode = encoder\n",
    "        self.decode = decoder\n",
    "        # 최대 디코딩 길이 설정\n",
    "        self.max_len = max_len\n",
    "        # 스페셜 토큰을 초기화에 입력하게 변경\n",
    "        self.spec_token = spec_token\n",
    "\n",
    "    def forward(self, src, tar=None, TF_ratio=1, BW=1):\n",
    "        # 인코더의 출력 = context_vector\n",
    "        context_h = self.encode(src)\n",
    "\n",
    "        # 스페셜 토큰에서 SOS, EOS, PAD의 정수인코딩값 추출\n",
    "        en_sos = self.spec_token.index('<SOS>')\n",
    "        en_eos = self.spec_token.index('<EOS>')\n",
    "        en_pad = self.spec_token.index('<PAD>')\n",
    "\n",
    "        if tar is not None:\n",
    "            # 배치사이즈, 연산위치 정보 추출(tar 기준으로)\n",
    "            BS, tar_seq_len = tar.size() \n",
    "            device = tar.device\n",
    "\n",
    "            # 디코더의 첫번째 토큰을 <SOS>에 (BS, 1)차원으로 생성\n",
    "            # 이때 정답지(tar)의 맨 앞토큰은 <SOS>로 채워져 있으니 이를 이용한다.\n",
    "            input_token = tar[:, 0].unsqueeze(1)\n",
    "            outputs = [] # 출력 디코드 결과를 저장\n",
    "\n",
    "            # 임의 난수를 (BS)차원으로 생성 후 TF_ratio비율정보를 받아서\n",
    "            # 마스크 플래그로 변환, 이때 TF_ratio는 0~1 사이값\n",
    "            # 1에 가까울수록 대부분의 Flag는 True가 되서 지도학습비율이 올라감\n",
    "            TF_flag = torch.rand(BS, device=device) < TF_ratio\n",
    "\n",
    "            # 토큰 단위로 예측이니 자가 회귀 방식임\n",
    "            h = context_h\n",
    "\n",
    "            # tar seq는 맨 처음 토큰을 <SOS>로 채웟으니 1번부터 시작\n",
    "            for t in range(1, tar_seq_len):\n",
    "                # 토큰단위로 입력이니 출력은 (bs, 1, tar_vocab)이다.\n",
    "                out_tokens, h = self.decode(input_token, h)\n",
    "                outputs.append(out_tokens)\n",
    "\n",
    "                # 마스크 플래그가 True : 지도학습 방식으로 동작\n",
    "                # 마스크 플래그가 False : 비지도학습-자가 회귀방식으로 동작\n",
    "                input_token = torch.where(TF_flag.unsqueeze(1), \n",
    "                                            tar[:, t].unsqueeze(1),\n",
    "                                            out_tokens.argmax(dim=-1))\n",
    "\n",
    "            # 최종 출력 모양 조정 \n",
    "            outputs = torch.cat(outputs, dim=1) # (BS, tar_seq_len-1, tar_vocab)\n",
    "            # (BS, 1, tar_vocab) 차원의 sos 토큰 인덱스로 채워진 텐서를 만듬\n",
    "            sos_tokens = torch.full((BS, 1, outputs.size(-1)), en_sos, device=device)\n",
    "            # sos_tokens랑 outputs를 합쳐서 (BS, tar_seq_len, tar_vocab)가 되게 함\n",
    "            outputs = torch.cat([sos_tokens, outputs], dim=1)\n",
    "            return outputs # (BS, tar_seq_len, tar_vocab)\n",
    "        \n",
    "        else: # 정답지가 없는 평가모드\n",
    "            # 배치사이즈, 연산위치 정보 추출(src 기준으로)\n",
    "            BS, src_seq_len = src.size() \n",
    "            device = src.device\n",
    "            # 최대 디코딩 길이 지정 안했으면 원문 seq_len을 쓰자\n",
    "            if self.max_len is None:\n",
    "                self.max_len = src_seq_len\n",
    "\n",
    "            # 디코더의 첫 토큰을 <SOS>에 (BS,1)로 채우고 맨 앞에 Beam_width적용\n",
    "            input_token = torch.full((BW, BS, 1), en_sos).to(device)\n",
    "            # 디코더 첫 토큰에 대한 확률정보 생성 (BW, BS, 1)\n",
    "            pred_probs = F.softmax(input_token.type(torch.float), dim=0)\n",
    "            # 최종 출력 시퀀스는 Beam_search의 후보군 시퀀스를 다 내보내는 것으로 함\n",
    "            # 후보 시퀀스를 다 내보낸 뒤 나중에 처리하는 것으로\n",
    "            outputs = torch.full((BS, self.max_len, BW), en_pad).to(device)\n",
    "\n",
    "            # <EOS>토큰을 각 beam search, batch_size에서 예측했으면 이를 감지하는 flag\n",
    "            pad_mask_flag = torch.zeros((BW, BS), dtype=torch.bool, \n",
    "                                        device=device)\n",
    "\n",
    "            # context_vector의 차원은 (num_layers, BS, hid_dim)임\n",
    "            # 이것을 (BW, num_layers, BS, hid_dim) 4차원으로 반복복제로 늘림\n",
    "            context_h = context_h.unsqueeze(0).repeat(BW, 1, 1, 1)\n",
    "\n",
    "            # 토큰 단위로 예측이니 자가 회귀 방식임\n",
    "            h = context_h\n",
    "\n",
    "            for t in range(self.max_len):\n",
    "                # 디코더에 입력하여 out_tokens를 만들어야 하니 입력 가능하게\n",
    "                # BW * BS를 곱해서 3차원으로 축소\n",
    "                input_token = input_token.view(BW*BS, -1) #(BW*BS, 1)\n",
    "                h = h.view(h.size(1), BW*BS, h.size(3)) # (nL, BW*BS, hid_dim)\n",
    "                \n",
    "                # 출력된 out_tokens의 차원은 (BW*BS, 1, tar_vocab)이다.\n",
    "                out_tokens, h = self.decode(input_token, h)\n",
    "                # h = h.view(BW, h.size(0), BS, -1)   # (BW, nL, BS, hid_dim)\n",
    "\n",
    "                # 원래 차원인 (BW, BS ,,)순으로 모두 복원\n",
    "                out_tokens = out_tokens.view(BW, BS, 1, -1) # (BW, BS, 1, tar_vocab)\n",
    "                # 모델이 예측한 토큰에 대한 확률정보 생성\n",
    "                cur_probs = F.softmax(out_tokens, dim=-1) # (BW, BS, 1, tar_vocab)\n",
    "\n",
    "                # cur_probs의 가장 높은 확률을 가진 BW(K)개 데이터 val, idx 추출\n",
    "                # val, idx의 BW개를 선택한 차원정보 : (BW, BS, 1, BW)\n",
    "                top_k_probs, top_k_idxs = torch.topk(input=cur_probs, k=BW, dim=-1)\n",
    "                \n",
    "                # pred_probs의 차원을 늘려서 (BW, BS, 1, 1)로 만든 다음\n",
    "                # 브로드 캐스팅 방법으로 top_k_probs와 곱한 뒤 (BW, BS, 1, BW)\n",
    "                # reshape를 적용하여 (BW*BW, BS, 1) 차원으로 변환\n",
    "                scores = (pred_probs.unsqueeze(-1) * top_k_probs).view(BW*BW, BS, 1)\n",
    "                # sorces 정보에 대해서 beam search로 BW개만큼의 데이터, idx 추출 (BW, BS, 1)\n",
    "                _, top_sorce_idx = torch.topk(input=scores, k=BW, dim=0)\n",
    "\n",
    "                # context vector 추적 및 추적정보를 바탕으로 갱신하기\n",
    "                re_h = h.permute(1, 0, 2) # (BW * BS, nL, hid_dim)\n",
    "                # (BW, BS, 1) -> (BW*BS, 1, 1) -> (BW*BS, nL, hid_dim)\n",
    "                track_h_idx = (top_sorce_idx // BW).reshape(-1, 1, 1).expand(-1, re_h.size(1), re_h.size(2))\n",
    "\n",
    "                re_h = torch.gather(re_h, 0, track_h_idx) # (BW*BS, nL, hid_dim)\n",
    "                # (BW*BS, nL, hid_dim) -> (nL, BW*BS, hid_dim) -> (BW, nL, BS, hid_dim)\n",
    "                h = re_h.permute(1, 0, 2).reshape(BW, h.size(0), BS, h.size(2))\n",
    "\n",
    "\n",
    "                # 이전토큰의 확률정보 pred_probs 업데이트\n",
    "                # cur_probs에서 상위 BW개를 추출한 top_k_probs (BW, BS, 1, BW)를\n",
    "                # (BW*BW, BS, 1)로 차원전환한 뒤 score의 BW개 idx 정보로 값 서치\n",
    "                # 따라서 업데이트 된 pred_probs는 (BW, BS, 1)차원으로 정상적으로 업데이트\n",
    "                pred_probs = torch.gather(top_k_probs.view(BW*BW, BS, 1), 0, top_sorce_idx)\n",
    "\n",
    "                # 입력토큰은 top_sorce_idx 정보를 바탕으로 top_k_idxs의 인덱스 정보를 찾으면 됨\n",
    "                input_token = torch.gather(top_k_idxs.view(BW*BW, BS, 1), 0, top_sorce_idx)\n",
    "\n",
    "                # out_tokes를 최종 출력물을 outputs의 t번째 seq에 덮어쓰기\n",
    "                outputs[:, t, :] = input_token.squeeze(-1).transpose(0, 1)  # (BS, max_len, BW)\n",
    "\n",
    "                # 업데이트한 input_token이 <EOS>토큰의 idx를 예측햇는지 검토\n",
    "                eos_indices = (input_token.squeeze(1) == en_eos)\n",
    "                # OR연산을 통해 pad_mask_flag가 <EOS>토큰의 idx를 예측했으면 TRUE로 전환한다\n",
    "                pad_mask_flag = pad_mask_flag | eos_indices\n",
    "\n",
    "                # 모든 샘플이 EOS예측\n",
    "                if pad_mask_flag.all():\n",
    "                    break\n",
    "\n",
    "            return outputs # (BS, max_len, BW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 실험 조건을 구분하기 위한 키\n",
    "model_key = ['LSTM', 'GRU']\n",
    "metrics_key = ['Loss', '정확도']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq의 LSTM버전 인스턴스화\n",
    "encoder_lstm = Encoder_LSTM(src_VS, EMB_DIM, UNIT_DIM, \n",
    "                            num_layer=NUM_Layers, bi=BI_DIR)\n",
    "decoder_lstm = Decoder_LSTM(tar_VS, EMB_DIM, UNIT_DIM,\n",
    "                            num_layer=NUM_Layers, bi=BI_DIR)\n",
    "Translater_lstm = Seq2Seq_LSTM(encoder_lstm, decoder_lstm, \n",
    "                               spec_token=spec_token, max_len=tar_SL)\n",
    "\n",
    "# Seq2Seq의 GRU버전 인스턴스화\n",
    "encoder_gru = Encoder_GRU(src_VS, EMB_DIM, UNIT_DIM, \n",
    "                            num_layer=NUM_Layers, bi=BI_DIR)\n",
    "decoder_gru = Decoder_GRU(tar_VS, EMB_DIM, UNIT_DIM,\n",
    "                            num_layer=NUM_Layers, bi=BI_DIR)\n",
    "Translater_gru = Seq2Seq_GRU(encoder_gru, decoder_gru, \n",
    "                             spec_token=spec_token, max_len=tar_SL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 저장\n",
    "path = {} #모델별 경로명 저장\n",
    "for mk in model_key:\n",
    "    path[mk] = f'Seq2Seq_{mk}.pth'\n",
    "    # torch.save(models[mk].state_dict(), path[mk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 불러오기\n",
    "load_model = {\n",
    "    'LSTM': Translater_lstm,  # LSTM 모델 인스턴스 생성\n",
    "    'GRU': Translater_gru     # GRU 모델 인스턴스 생성\n",
    "}\n",
    "for mk in model_key:\n",
    "    load_model[mk].load_state_dict(torch.load(path[mk], weights_only=True))\n",
    "    #추론기는 CPU에서 돌리자\n",
    "    load_model[mk] = load_model[mk].to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# 테스트 데이터셋에서 샘플을 추출\n",
    "# 전체 테스트 데이터 개수정보를 추출\n",
    "num_test = padded_src_test.shape[0]\n",
    "sample_epoch = 5 #추출할 샘플 개수 정의\n",
    "indices = random.sample(range(num_test), sample_epoch)\n",
    "\n",
    "# 추출한 샘플번호를 바탕으로 Test 데이터셋에서 무작위 추출\n",
    "S_src_test = padded_src_test[indices]\n",
    "S_tar_test = padded_tar_test[indices]\n",
    "\n",
    "# 원문 데이터만 텐서 자료형으로 변환\n",
    "TS_src_test = torch.tensor(S_src_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--현재 추론 조건: [Seq2Seq_LSTM]--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--현재 추론 조건: [Seq2Seq_GRU]--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 16.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from Seq_trainer import *\n",
    "\n",
    "# 추론 결과를 저장할 딕셔너리\n",
    "tar_dict = {key: []  for key in model_key}\n",
    "\n",
    "BW = 3 #Beam Search의 Search Space 계수(Beam_width)값\n",
    "\n",
    "for key in model_key:\n",
    "    print(f\"\\n--현재 추론 조건: [Seq2Seq_{key}]--\") # 조건에 맞는 실험시작\n",
    "    for idx in tqdm(range(sample_epoch)): #추론 에포크별 추론 시작\n",
    "        # 입력되는 원문 차원을 (1, src_seq_len)으로 만들기 위한 코드\n",
    "        iter_src_data = TS_src_test[idx].unsqueeze(0)\n",
    "        # 모델 추론 -> 추론결과는 (bs, max_len, BW) ndarray타입임\n",
    "        # BW 옵션이 1 -> 사실상 Greedy Search랑 같은 결과\n",
    "        # BW 옵션이 1 이상 -> 제대로된 Beam Search 작업수행\n",
    "        tar_infer_doc = model_inference(load_model[key], \n",
    "                                        iter_src_data, BW=BW)\n",
    "        tar_dict[key].append(tar_infer_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 번역 결과 확인\n",
      "원문(src) : dann nehmen sie die jungen weg .\n",
      "\n",
      "seqLSTM_0_번역 : 그런 다음 그들은 함께 일했습니다 . . .\n",
      "seq-GRU_0_번역 : 그러면0살되어 있습니다\n",
      "seqLSTM_1_번역 : 그런 다음 그들은 함께 일했습니다 . 있습니다 . .\n",
      "seq-GRU_1_번역 : 그러면 그들은명의됩니다 . .\n",
      "seqLSTM_2_번역 : 처리가 그녀는 약속을 돌하게 살고 있습니다 있습니다 . 있습니다\n",
      "seq-GRU_2_번역 : 00 그들은들되었다 .\n",
      "\n",
      "번역문(tar) : 그런 다음 그들은 새끼들을 데려갑니다 .\n",
      "==============================\n",
      "\n",
      "2번째 번역 결과 확인\n",
      "원문(src) : seit geraumer zeit wird sie von ihrem mann vernachlassigt und genau das beabsichtigt sie mittels ihrer wurzigen kochkunst zu andern .\n",
      "\n",
      "seqLSTM_0_번역 : 형식의 접목 처리가 여유회사의 모의 그녀의 인내를 그들의 그들의 함께 그들이놓을 목표를 대해하고 있습니다 .는 .\n",
      "seq-GRU_0_번역 : 그녀의 주요 번째 주요 순간 , 즉 , 없는 동안 ,하는 것은 , 여성의 그들의 오랜적 재능으로 아니라 , 되돌아이 .으로 결정 .\n",
      "seqLSTM_1_번역 : 분배 처리가선거 그녀의 마지막에 대한 그들의 이익을 위해 그들은과 그들의 그들은 놓는 것에 달성 이야기하고 있습니다 . .자 .는 것입니다 . .\n",
      "seq-GRU_1_번역 : 그녀의 주요 순간 방향점은 , 그러나 정기적으로 겨울 시대에 회복흔든 , 의사는 그들의 음악 보석이고 집주인에게흙었습니다 되었습니다 .도 데려했습니다 .\n",
      "seqLSTM_2_번역 : 데스는 이미 처리가 출생을 그녀가 봤던 감정을심을 그들 삶을 영원히 바꿔 놓 신호들을 이야기하는 것을 발견합니다 )고 . .는\n",
      "seq-GRU_2_번역 : 보트 첫 순간 중에는 진 여성이 혼란이 진행되는과 뒤 아이도는 리 루이스들은 폭 주요 시간 수준을이의 진 쥐 행운입니다 .도에 합니다합니다 .\n",
      "\n",
      "번역문(tar) : 오랫동안 남편에게 외면당했던 그녀는 향기로운 요리를 통해 그것을 변화시키려는 것이다 .\n",
      "==============================\n",
      "\n",
      "3번째 번역 결과 확인\n",
      "원문(src) : biagini suchte 45 minuten lang nach ihm , bevor der wal nur etwa 180 meter von seinem bug entfernt auftauchte .\n",
      "\n",
      "seqLSTM_0_번역 : 분배 처리가 10억 . . .\n",
      "seq-GRU_0_번역 : 약방구가 2000년 전에 대부분의 가이드 , 터널 표시를서 10분해 왔습니다 . .에 가장\n",
      "seqLSTM_1_번역 : 처리가두기 원 원 . . .\n",
      "seq-GRU_1_번역 : 약탈은 2000년 전에 대부분의 가이드 , 터널 조사에서 10분 안에 10분에 있습니다에도 가장\n",
      "seqLSTM_2_번역 : 내용물을이이 . . .\n",
      "seq-GRU_2_번역 : 석탈은 약년마다 월 바다가 10분 조사에 약 20분 안에 있을 수 있었습니다 . 가장에\n",
      "\n",
      "번역문(tar) : 비아기니는 고래가 뱃서 불과 180m 떨어진 지점에 나타나기 전까지 45분 동안 고래를 찾았습니다 .\n",
      "==============================\n",
      "\n",
      "4번째 번역 결과 확인\n",
      "원문(src) : es ist die energie , die außergewohnliche interviews und außergewohnliche leben entstehen lasst .\n",
      "\n",
      "seqLSTM_0_번역 : 소비 측면에서 약탈 % 는 이미 사건을 있습니다 . .\n",
      "seq-GRU_0_번역 : 5가지 혁신적인 요소와 죽음 묘사 알려져 있습니다 .에입니다 .\n",
      "seqLSTM_1_번역 : 에너지 소비 측면에서 약탈에 대한 일련의 사건을 해결합니다 . .\n",
      "seq-GRU_1_번역 : 5가지 구성 요소와 죽음이 알려져 있습니다 .에 중요합니다 .\n",
      "seqLSTM_2_번역 : 에너지렸다에 있는 95에 대한 일련의 자라고 해결합니다 .는\n",
      "seq-GRU_2_번역 : 모든 구성 문화를에 의해이된 겸 겸 구성 요소 중요합니다 .\n",
      "\n",
      "번역문(tar) : 그것은 특별한 인터뷰와 특별한 삶을 만드는 에너지입니다 .\n",
      "==============================\n",
      "\n",
      "5번째 번역 결과 확인\n",
      "원문(src) : aber seine erste anstellung war als theaterproduzent .\n",
      "\n",
      "seqLSTM_0_번역 : 그러나 그의 줘야 10억 10억 . . . . .\n",
      "seq-GRU_0_번역 : 그러나 그의 첫 번째 번째 영화자 피터가이다 .\n",
      "seqLSTM_1_번역 : 그러나 그의 전문 불가능 말입니다니다 . . .\n",
      "seq-GRU_1_번역 : 그러나 그의 첫 번째 영화 제작자 피터가가 되었습니다 .\n",
      "seqLSTM_2_번역 : 전문 분야십시오 . . . . . . .\n",
      "seq-GRU_2_번역 : 대사 그는 그의 첫 영화 제작 제작자 피터b가 되었습니다 .\n",
      "\n",
      "번역문(tar) : 하지만 그의 첫 직업은 연극 제작자였습니다 .\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx_list = range(1, sample_epoch+1)\n",
    "for idx, src, pred_lstm, pred_gru, tar in zip(idx_list, S_src_test, \n",
    "                                              tar_dict['LSTM'], \n",
    "                                              tar_dict['GRU'], \n",
    "                                              S_tar_test):\n",
    "    # 원문, 모델번역문_1, 모델번역문_2, 정답번역문 순으로 디코딩\n",
    "\n",
    "    decode_src = Translater_post_processor(src, idx_to_src, spec_token)\n",
    "    \n",
    "    de_pred_lstm_list = [] #Beam_search를 수행함으로 인한 후보군 데이터를 저장하는 리스트 \n",
    "    de_pred_gru_list = []  #Beam_search를 수행함으로 인한 후보군 데이터를 저장하는 리스트\n",
    "\n",
    "    for BW_i in range(BW):\n",
    "        de_pred_lstm = Translater_post_processor(pred_lstm[:, BW_i], idx_to_tar, spec_token)\n",
    "        de_pred_gru = Translater_post_processor(pred_gru[:, BW_i], idx_to_tar, spec_token)\n",
    "        \n",
    "        de_pred_lstm_list.append(de_pred_lstm)\n",
    "        de_pred_gru_list.append(de_pred_gru)\n",
    "\n",
    "    decode_tar = Translater_post_processor(tar, idx_to_tar, spec_token)\n",
    "\n",
    "    print(f\"{idx}번째 번역 결과 확인\")\n",
    "    print(f\"원문(src) : {decode_src}\", end='\\n\\n')\n",
    "\n",
    "    for BW_i in range(BW):\n",
    "        print(f\"seqLSTM_{BW_i}_번역 : {de_pred_lstm_list[BW_i]}\")\n",
    "        print(f\"seq-GRU_{BW_i}_번역 : {de_pred_gru_list[BW_i]}\")\n",
    "\n",
    "    print(f\"\\n번역문(tar) : {decode_tar}\")\n",
    "    print(\"==============================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.special import softmax\n",
    "\n",
    "def BLEU_Score(candidate, reference, p_list=None, ignore_idx=0):\n",
    "    if p_list is None:\n",
    "        p_list = [0.25, 0.25, 0.25, 0.25]  # 기본적으로 균등 가중치 설정\n",
    "    \n",
    "    def n_gram_precision(candidate, reference, n, ignore_idx):\n",
    "        candidate_n_grams = [tuple(candidate[i:i+n]) \n",
    "                             for i in range(len(candidate) - n + 1) \n",
    "                                if ignore_idx not in candidate[i:i+n]]\n",
    "        reference_n_grams = [tuple(reference[i:i+n]) \n",
    "                             for i in range(len(reference) - n + 1) \n",
    "                                if ignore_idx not in reference[i:i+n]]\n",
    "        \n",
    "        candidate_counter = Counter(candidate_n_grams)\n",
    "        reference_counter = Counter(reference_n_grams)\n",
    "        \n",
    "        overlap = sum(min(candidate_counter[ng], reference_counter[ng]) \n",
    "                                for ng in candidate_counter)\n",
    "        total = sum(candidate_counter.values())\n",
    "        \n",
    "        if total == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return overlap / total\n",
    "\n",
    "\n",
    "    best_bleu_score = 0\n",
    "    best_candidate = None\n",
    "    bleu_score_list = []\n",
    "\n",
    "    for i in range(candidate.shape[1]):  # BW 개수만큼 후보군을 반복\n",
    "        current_candidate = candidate[:, i]\n",
    "        precisions = []\n",
    "        \n",
    "        for n in range(1, 5):  # 1-gram, 2-gram, 3-gram, 4-gram\n",
    "            p_n = n_gram_precision(current_candidate, reference, n, ignore_idx)\n",
    "            # Smoothing: precision이 0인 경우 작은 값(예: 1e-9)으로 대체\n",
    "            precisions.append(p_n if p_n > 0 else 1e-9)\n",
    "        \n",
    "        log_precisions = [p_list[n-1] * np.log(p) for n, p in \n",
    "                            enumerate(precisions, start=1)]\n",
    "        bleu_score = np.exp(sum(log_precisions))\n",
    "\n",
    "        bleu_score_list.append(bleu_score*(10^20))\n",
    "        \n",
    "\n",
    "    # bleu_score_result의 값이 원체 작아서 softmax 처리\n",
    "    bleu_score_list = softmax(bleu_score_list)\n",
    "\n",
    "    # 가장 높은 BLEU score를 가진 후보군 선택\n",
    "    best_idx = np.argmax(bleu_score_list)\n",
    "    best_bleu_score = bleu_score_list[best_idx]\n",
    "    best_candidate = candidate[:, best_idx]\n",
    "    \n",
    "    bleu_dict = {\n",
    "        '최고BLUE점수': f'{best_bleu_score * 100:.2f}%',\n",
    "        'Best_y예측': best_candidate\n",
    "    }\n",
    "    \n",
    "    return bleu_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 번역 결과 확인\n",
      "원문(src) : dann nehmen sie die jungen weg .\n",
      "seqLSTM_점수: 33.94%\n",
      "seqLSTM_번역: 그런 다음 그들은 함께 일했습니다 . . .\n",
      "seqGRU_점수: 33.34%\n",
      "seqGRU_번역: 그러면 그들은명의됩니다 . .\n",
      "번역문(tar) : 그런 다음 그들은 새끼들을 데려갑니다 .\n",
      "==============================\n",
      "\n",
      "2번째 번역 결과 확인\n",
      "원문(src) : seit geraumer zeit wird sie von ihrem mann vernachlassigt und genau das beabsichtigt sie mittels ihrer wurzigen kochkunst zu andern .\n",
      "seqLSTM_점수: 33.33%\n",
      "seqLSTM_번역: 형식의 접목 처리가 여유회사의 모의 그녀의 인내를 그들의 그들의 함께 그들이놓을 목표를 대해하고 있습니다 .는 .\n",
      "seqGRU_점수: 33.33%\n",
      "seqGRU_번역: 그녀의 주요 번째 주요 순간 , 즉 , 없는 동안 ,하는 것은 , 여성의 그들의 오랜적 재능으로 아니라 , 되돌아이 .으로 결정 .\n",
      "번역문(tar) : 오랫동안 남편에게 외면당했던 그녀는 향기로운 요리를 통해 그것을 변화시키려는 것이다 .\n",
      "==============================\n",
      "\n",
      "3번째 번역 결과 확인\n",
      "원문(src) : biagini suchte 45 minuten lang nach ihm , bevor der wal nur etwa 180 meter von seinem bug entfernt auftauchte .\n",
      "seqLSTM_점수: 33.33%\n",
      "seqLSTM_번역: 분배 처리가 10억 . . .\n",
      "seqGRU_점수: 33.34%\n",
      "seqGRU_번역: 약방구가 2000년 전에 대부분의 가이드 , 터널 표시를서 10분해 왔습니다 . .에 가장\n",
      "번역문(tar) : 비아기니는 고래가 뱃서 불과 180m 떨어진 지점에 나타나기 전까지 45분 동안 고래를 찾았습니다 .\n",
      "==============================\n",
      "\n",
      "4번째 번역 결과 확인\n",
      "원문(src) : es ist die energie , die außergewohnliche interviews und außergewohnliche leben entstehen lasst .\n",
      "seqLSTM_점수: 33.34%\n",
      "seqLSTM_번역: 에너지 소비 측면에서 약탈에 대한 일련의 사건을 해결합니다 . .\n",
      "seqGRU_점수: 34.25%\n",
      "seqGRU_번역: 5가지 혁신적인 요소와 죽음 묘사 알려져 있습니다 .에입니다 .\n",
      "번역문(tar) : 그것은 특별한 인터뷰와 특별한 삶을 만드는 에너지입니다 .\n",
      "==============================\n",
      "\n",
      "5번째 번역 결과 확인\n",
      "원문(src) : aber seine erste anstellung war als theaterproduzent .\n",
      "seqLSTM_점수: 33.33%\n",
      "seqLSTM_번역: 그러나 그의 줘야 10억 10억 . . . . .\n",
      "seqGRU_점수: 33.33%\n",
      "seqGRU_번역: 그러나 그의 첫 번째 영화 제작자 피터가가 되었습니다 .\n",
      "번역문(tar) : 하지만 그의 첫 직업은 연극 제작자였습니다 .\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BLEU score를 적용하여 원문 / 예측구문 / 번역문 결과 확인\n",
    "idx_list = range(1, sample_epoch+1)\n",
    "for idx, src, pred_lstm, pred_gru, tar in zip(idx_list, S_src_test, \n",
    "                                              tar_dict['LSTM'], \n",
    "                                              tar_dict['GRU'], \n",
    "                                              S_tar_test):\n",
    "    # 원문, 모델번역문_1, 모델번역문_2, 정답번역문 순으로 디코딩\n",
    "    decode_src = Translater_post_processor(src, idx_to_src, spec_token)\n",
    "\n",
    "    # 모델 예측 결과물에 대하여 BLEU 스코어 및 Best_y_pred 추출\n",
    "    BLEU_LSTM = BLEU_Score(pred_lstm, tar)\n",
    "    BLEU_LSTM_score = BLEU_LSTM['최고BLUE점수']\n",
    "    b_de_pred_lstm = Translater_post_processor(BLEU_LSTM['Best_y예측'], idx_to_tar, spec_token)\n",
    "    BLEU_GRU = BLEU_Score(pred_gru, tar)\n",
    "    BLEU_GRU_score = BLEU_GRU['최고BLUE점수']\n",
    "    b_de_pred_gru = Translater_post_processor(BLEU_GRU['Best_y예측'], idx_to_tar, spec_token)\n",
    "\n",
    "    decode_tar = Translater_post_processor(tar, idx_to_tar, spec_token)\n",
    "\n",
    "    print(f\"{idx}번째 번역 결과 확인\")\n",
    "    print(f\"원문(src) : {decode_src}\", end='\\n')\n",
    "\n",
    "    print(f\"seqLSTM_점수: {BLEU_LSTM_score}\\nseqLSTM_번역: {b_de_pred_lstm}\")\n",
    "    print(f\"seqGRU_점수: {BLEU_GRU_score}\\nseqGRU_번역: {b_de_pred_gru}\")\n",
    "\n",
    "    print(f\"번역문(tar) : {decode_tar}\")\n",
    "    print(\"==============================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
